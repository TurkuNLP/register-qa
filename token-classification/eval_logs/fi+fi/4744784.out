START 4744784: Thu 12 Oct 2023 11:09:47 AM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/annotated/train_annotated_dataset.jsonl'], test='../../data/qa_token_classification/annotated/test_annotated_dataset.jsonl', dev='../../data/qa_token_classification/annotated/dev_annotated_dataset.jsonl', batch=8, epochs=10, lr=5e-05, save=None, dataset=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
Downloading and preparing dataset json/default to /users/annieske/.cache/huggingface/datasets/json/default-ef26603b715bf39d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...
Dataset json downloaded and prepared to /users/annieske/.cache/huggingface/datasets/json/default-ef26603b715bf39d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
    validation: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 50
    })
    test: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 68
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'loss': 1.0006, 'learning_rate': 4.038461538461539e-05, 'epoch': 1.92}
              precision    recall  f1-score   support

       NSWER       0.00      0.00      0.00        40
     UESTION       0.00      0.00      0.00        46

   micro avg       0.00      0.00      0.00        86
   macro avg       0.00      0.00      0.00        86
weighted avg       0.00      0.00      0.00        86

{'eval_loss': 0.9457056522369385, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.4621614781068084, 'eval_runtime': 1.5606, 'eval_samples_per_second': 32.038, 'eval_steps_per_second': 2.563, 'epoch': 1.92}
{'loss': 0.6611, 'learning_rate': 3.0769230769230774e-05, 'epoch': 3.85}
              precision    recall  f1-score   support

       NSWER       0.06      0.30      0.10        40
     UESTION       0.11      0.37      0.17        46

   micro avg       0.08      0.34      0.13        86
   macro avg       0.08      0.33      0.13        86
weighted avg       0.08      0.34      0.13        86

{'eval_loss': 0.8506790995597839, 'eval_precision': 0.07945205479452055, 'eval_recall': 0.3372093023255814, 'eval_f1': 0.12860310421286034, 'eval_accuracy': 0.6526195899772209, 'eval_runtime': 0.5577, 'eval_samples_per_second': 89.646, 'eval_steps_per_second': 7.172, 'epoch': 3.85}
{'loss': 0.3915, 'learning_rate': 2.1153846153846154e-05, 'epoch': 5.77}
              precision    recall  f1-score   support

       NSWER       0.07      0.30      0.12        40
     UESTION       0.14      0.37      0.21        46

   micro avg       0.10      0.34      0.16        86
   macro avg       0.11      0.33      0.16        86
weighted avg       0.11      0.34      0.16        86

{'eval_loss': 0.9452129602432251, 'eval_precision': 0.10175438596491228, 'eval_recall': 0.3372093023255814, 'eval_f1': 0.15633423180592992, 'eval_accuracy': 0.6704631738800304, 'eval_runtime': 0.5837, 'eval_samples_per_second': 85.66, 'eval_steps_per_second': 6.853, 'epoch': 5.77}
{'loss': 0.157, 'learning_rate': 1.153846153846154e-05, 'epoch': 7.69}
              precision    recall  f1-score   support

       NSWER       0.14      0.47      0.22        40
     UESTION       0.23      0.52      0.32        46

   micro avg       0.18      0.50      0.26        86
   macro avg       0.18      0.50      0.27        86
weighted avg       0.19      0.50      0.27        86

{'eval_loss': 1.0350127220153809, 'eval_precision': 0.17916666666666667, 'eval_recall': 0.5, 'eval_f1': 0.26380368098159507, 'eval_accuracy': 0.7179195140470767, 'eval_runtime': 0.5644, 'eval_samples_per_second': 88.587, 'eval_steps_per_second': 7.087, 'epoch': 7.69}
{'loss': 0.0582, 'learning_rate': 1.9230769230769234e-06, 'epoch': 9.62}
              precision    recall  f1-score   support

       NSWER       0.18      0.50      0.26        40
     UESTION       0.31      0.57      0.40        46

   micro avg       0.23      0.53      0.32        86
   macro avg       0.24      0.53      0.33        86
weighted avg       0.25      0.53      0.33        86

{'eval_loss': 1.1268539428710938, 'eval_precision': 0.23115577889447236, 'eval_recall': 0.5348837209302325, 'eval_f1': 0.32280701754385965, 'eval_accuracy': 0.7250063275120223, 'eval_runtime': 0.5666, 'eval_samples_per_second': 88.253, 'eval_steps_per_second': 7.06, 'epoch': 9.62}
{'train_runtime': 45.0233, 'train_samples_per_second': 22.211, 'train_steps_per_second': 2.887, 'train_loss': 0.4368412823631213, 'epoch': 10.0}
              precision    recall  f1-score   support

       NSWER       0.03      0.20      0.06        59
     UESTION       0.10      0.40      0.16        73

   micro avg       0.06      0.31      0.11       132
   macro avg       0.07      0.30      0.11       132
weighted avg       0.07      0.31      0.12       132

{'epoch': 10.0,
 'eval_accuracy': 0.6659752993306307,
 'eval_f1': 0.10718954248366012,
 'eval_loss': 0.8395266532897949,
 'eval_precision': 0.06477093206951026,
 'eval_recall': 0.3106060606060606,
 'eval_runtime': 0.8944,
 'eval_samples_per_second': 76.032,
 'eval_steps_per_second': 5.591}
Accuracy: 0.6659752993306307
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	5e-5	num_train_epochs	10	END 4744784: Thu 12 Oct 2023 11:10:52 AM EEST
