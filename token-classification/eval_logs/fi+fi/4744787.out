START 4744787: Thu 12 Oct 2023 11:10:03 AM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/annotated/train_annotated_dataset.jsonl'], test='../../data/qa_token_classification/annotated/test_annotated_dataset.jsonl', dev='../../data/qa_token_classification/annotated/dev_annotated_dataset.jsonl', batch=8, epochs=10, lr=1e-05, save=None, dataset=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
    validation: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 50
    })
    test: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 68
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'loss': 1.0837, 'learning_rate': 8.076923076923077e-06, 'epoch': 1.92}
              precision    recall  f1-score   support

       NSWER       0.00      0.00      0.00        40
     UESTION       0.00      0.00      0.00        46

   micro avg       0.00      0.00      0.00        86
   macro avg       0.00      0.00      0.00        86
weighted avg       0.00      0.00      0.00        86

{'eval_loss': 1.0153264999389648, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.45494811440141736, 'eval_runtime': 0.5257, 'eval_samples_per_second': 95.117, 'eval_steps_per_second': 7.609, 'epoch': 1.92}
{'loss': 0.9188, 'learning_rate': 6.153846153846155e-06, 'epoch': 3.85}
              precision    recall  f1-score   support

       NSWER       0.00      0.05      0.01        40
     UESTION       0.00      0.00      0.00        46

   micro avg       0.00      0.02      0.00        86
   macro avg       0.00      0.03      0.00        86
weighted avg       0.00      0.02      0.00        86

{'eval_loss': 0.9068575501441956, 'eval_precision': 0.0023752969121140144, 'eval_recall': 0.023255813953488372, 'eval_f1': 0.004310344827586207, 'eval_accuracy': 0.5321437610731461, 'eval_runtime': 0.5729, 'eval_samples_per_second': 87.282, 'eval_steps_per_second': 6.983, 'epoch': 3.85}
{'loss': 0.7577, 'learning_rate': 4.230769230769231e-06, 'epoch': 5.77}
              precision    recall  f1-score   support

       NSWER       0.01      0.07      0.01        40
     UESTION       0.00      0.00      0.00        46

   micro avg       0.00      0.03      0.01        86
   macro avg       0.00      0.04      0.01        86
weighted avg       0.00      0.03      0.01        86

{'eval_loss': 0.8610318899154663, 'eval_precision': 0.004291845493562232, 'eval_recall': 0.03488372093023256, 'eval_f1': 0.007643312101910829, 'eval_accuracy': 0.5946595798532017, 'eval_runtime': 0.6876, 'eval_samples_per_second': 72.719, 'eval_steps_per_second': 5.817, 'epoch': 5.77}
{'loss': 0.6061, 'learning_rate': 2.307692307692308e-06, 'epoch': 7.69}
              precision    recall  f1-score   support

       NSWER       0.02      0.12      0.03        40
     UESTION       0.00      0.00      0.00        46

   micro avg       0.01      0.06      0.02        86
   macro avg       0.01      0.06      0.01        86
weighted avg       0.01      0.06      0.01        86

{'eval_loss': 0.8761375546455383, 'eval_precision': 0.009433962264150943, 'eval_recall': 0.05813953488372093, 'eval_f1': 0.016233766233766232, 'eval_accuracy': 0.5976967856238927, 'eval_runtime': 0.5538, 'eval_samples_per_second': 90.277, 'eval_steps_per_second': 7.222, 'epoch': 7.69}
{'loss': 0.5201, 'learning_rate': 3.846153846153847e-07, 'epoch': 9.62}
              precision    recall  f1-score   support

       NSWER       0.02      0.12      0.04        40
     UESTION       0.01      0.02      0.01        46

   micro avg       0.02      0.07      0.02        86
   macro avg       0.01      0.07      0.02        86
weighted avg       0.01      0.07      0.02        86

{'eval_loss': 0.886596143245697, 'eval_precision': 0.015037593984962405, 'eval_recall': 0.06976744186046512, 'eval_f1': 0.024742268041237112, 'eval_accuracy': 0.611364211592002, 'eval_runtime': 0.5484, 'eval_samples_per_second': 91.172, 'eval_steps_per_second': 7.294, 'epoch': 9.62}
{'train_runtime': 45.5533, 'train_samples_per_second': 21.952, 'train_steps_per_second': 2.854, 'train_loss': 0.7659465606396015, 'epoch': 10.0}
              precision    recall  f1-score   support

       NSWER       0.01      0.07      0.01        59
     UESTION       0.00      0.00      0.00        73

   micro avg       0.00      0.03      0.01       132
   macro avg       0.00      0.03      0.01       132
weighted avg       0.00      0.03      0.01       132

{'epoch': 10.0,
 'eval_accuracy': 0.5564250023569342,
 'eval_f1': 0.007272727272727274,
 'eval_loss': 0.9134585857391357,
 'eval_precision': 0.004132231404958678,
 'eval_recall': 0.030303030303030304,
 'eval_runtime': 0.7134,
 'eval_samples_per_second': 95.321,
 'eval_steps_per_second': 7.009}
Accuracy: 0.5564250023569342
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	1e-5	num_train_epochs	10	END 4744787: Thu 12 Oct 2023 11:11:12 AM EEST
