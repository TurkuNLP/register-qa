START 4744791: Thu 12 Oct 2023 11:10:42 AM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/annotated/train_annotated_dataset.jsonl', '../../data/qa_token_classification/annotated/cleaned2_chatgpt_annotations_dataset.jsonl'], test='../../data/qa_token_classification/annotated/test_annotated_dataset.jsonl', dev='../../data/qa_token_classification/annotated/dev_annotated_dataset.jsonl', batch=8, epochs=10, lr=2e-05, save=None, dataset=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
in dictionary: 1
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 3424
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 3524
    })
})
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 3524
    })
    validation: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 50
    })
    test: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 68
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'loss': 0.8422, 'learning_rate': 1.886621315192744e-05, 'epoch': 0.57}
              precision    recall  f1-score   support

       NSWER       0.08      0.35      0.13        40
     UESTION       0.16      0.46      0.23        46

   micro avg       0.11      0.41      0.18        86
   macro avg       0.12      0.40      0.18        86
weighted avg       0.12      0.41      0.19        86

{'eval_loss': 0.4837314486503601, 'eval_precision': 0.11475409836065574, 'eval_recall': 0.4069767441860465, 'eval_f1': 0.1790281329923274, 'eval_accuracy': 0.8325740318906606, 'eval_runtime': 0.5678, 'eval_samples_per_second': 88.061, 'eval_steps_per_second': 7.045, 'epoch': 0.57}
{'loss': 0.6484, 'learning_rate': 1.7732426303854877e-05, 'epoch': 1.13}
              precision    recall  f1-score   support

       NSWER       0.12      0.23      0.15        40
     UESTION       0.22      0.43      0.30        46

   micro avg       0.17      0.34      0.23        86
   macro avg       0.17      0.33      0.23        86
weighted avg       0.17      0.34      0.23        86

{'eval_loss': 0.5985957980155945, 'eval_precision': 0.1746987951807229, 'eval_recall': 0.3372093023255814, 'eval_f1': 0.23015873015873017, 'eval_accuracy': 0.7177929638066313, 'eval_runtime': 0.4979, 'eval_samples_per_second': 100.417, 'eval_steps_per_second': 8.033, 'epoch': 1.13}
{'loss': 0.5783, 'learning_rate': 1.6598639455782314e-05, 'epoch': 1.7}
              precision    recall  f1-score   support

       NSWER       0.12      0.30      0.17        40
     UESTION       0.20      0.33      0.25        46

   micro avg       0.15      0.31      0.20        86
   macro avg       0.16      0.31      0.21        86
weighted avg       0.16      0.31      0.21        86

{'eval_loss': 0.5064670443534851, 'eval_precision': 0.15168539325842698, 'eval_recall': 0.313953488372093, 'eval_f1': 0.20454545454545453, 'eval_accuracy': 0.7672741078208049, 'eval_runtime': 0.5026, 'eval_samples_per_second': 99.488, 'eval_steps_per_second': 7.959, 'epoch': 1.7}
{'loss': 0.5407, 'learning_rate': 1.546485260770975e-05, 'epoch': 2.27}
              precision    recall  f1-score   support

       NSWER       0.23      0.38      0.29        40
     UESTION       0.34      0.52      0.41        46

   micro avg       0.29      0.45      0.35        86
   macro avg       0.29      0.45      0.35        86
weighted avg       0.29      0.45      0.35        86

{'eval_loss': 0.6354832053184509, 'eval_precision': 0.28888888888888886, 'eval_recall': 0.45348837209302323, 'eval_f1': 0.35294117647058815, 'eval_accuracy': 0.7877752467729688, 'eval_runtime': 0.5307, 'eval_samples_per_second': 94.215, 'eval_steps_per_second': 7.537, 'epoch': 2.27}
{'loss': 0.4764, 'learning_rate': 1.433106575963719e-05, 'epoch': 2.83}
              precision    recall  f1-score   support

       NSWER       0.24      0.47      0.32        40
     UESTION       0.25      0.39      0.31        46

   micro avg       0.24      0.43      0.31        86
   macro avg       0.24      0.43      0.31        86
weighted avg       0.24      0.43      0.31        86

{'eval_loss': 0.5523971319198608, 'eval_precision': 0.24342105263157895, 'eval_recall': 0.43023255813953487, 'eval_f1': 0.3109243697478991, 'eval_accuracy': 0.7589217919514047, 'eval_runtime': 0.547, 'eval_samples_per_second': 91.404, 'eval_steps_per_second': 7.312, 'epoch': 2.83}
{'loss': 0.395, 'learning_rate': 1.3197278911564626e-05, 'epoch': 3.4}
              precision    recall  f1-score   support

       NSWER       0.12      0.38      0.19        40
     UESTION       0.23      0.39      0.29        46

   micro avg       0.17      0.38      0.23        86
   macro avg       0.18      0.38      0.24        86
weighted avg       0.18      0.38      0.24        86

{'eval_loss': 0.7342638373374939, 'eval_precision': 0.16751269035532995, 'eval_recall': 0.38372093023255816, 'eval_f1': 0.2332155477031802, 'eval_accuracy': 0.7090609972158947, 'eval_runtime': 0.5142, 'eval_samples_per_second': 97.232, 'eval_steps_per_second': 7.779, 'epoch': 3.4}
{'train_runtime': 282.732, 'train_samples_per_second': 124.641, 'train_steps_per_second': 15.598, 'train_loss': 0.5801549377441406, 'epoch': 3.4}
              precision    recall  f1-score   support

       NSWER       0.14      0.41      0.21        59
     UESTION       0.17      0.41      0.24        73

   micro avg       0.16      0.41      0.23       132
   macro avg       0.16      0.41      0.23       132
weighted avg       0.16      0.41      0.23       132

{'epoch': 3.4,
 'eval_accuracy': 0.8023946450457246,
 'eval_f1': 0.22594142259414227,
 'eval_loss': 0.49717622995376587,
 'eval_precision': 0.15606936416184972,
 'eval_recall': 0.4090909090909091,
 'eval_runtime': 0.7426,
 'eval_samples_per_second': 91.576,
 'eval_steps_per_second': 6.734}
Accuracy: 0.8023946450457246
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	2e-5	num_train_epochs	10	END 4744791: Thu 12 Oct 2023 11:16:13 AM EEST
