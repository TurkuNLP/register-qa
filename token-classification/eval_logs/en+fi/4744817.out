START 4744817: Thu 12 Oct 2023 11:14:54 AM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/dataset_punct2'], test='../../data/qa_token_classification/annotated/test_annotated_dataset.jsonl', dev='../../data/qa_token_classification/annotated/dev_annotated_dataset.jsonl', batch=8, epochs=10, lr=2e-05, save=None, dataset=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 134658
    })
    validation: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 1683
    })
    test: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 5693
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 134658
    })
})
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 134658
    })
    validation: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 50
    })
    test: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 68
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'loss': 0.107, 'learning_rate': 1.9702964415136935e-05, 'epoch': 0.15}
              precision    recall  f1-score   support

       NSWER       0.12      0.20      0.15        40
     UESTION       0.01      0.02      0.02        46

   micro avg       0.06      0.10      0.08        86
   macro avg       0.07      0.11      0.08        86
weighted avg       0.06      0.10      0.08        86

{'eval_loss': 4.369218349456787, 'eval_precision': 0.06428571428571428, 'eval_recall': 0.10465116279069768, 'eval_f1': 0.07964601769911503, 'eval_accuracy': 0.44874715261959, 'eval_runtime': 0.6036, 'eval_samples_per_second': 82.838, 'eval_steps_per_second': 6.627, 'epoch': 0.15}
{'loss': 0.066, 'learning_rate': 1.940592883027387e-05, 'epoch': 0.3}
              precision    recall  f1-score   support

       NSWER       0.08      0.15      0.10        40
     UESTION       0.03      0.04      0.03        46

   micro avg       0.05      0.09      0.07        86
   macro avg       0.05      0.10      0.07        86
weighted avg       0.05      0.09      0.06        86

{'eval_loss': 5.486764430999756, 'eval_precision': 0.05161290322580645, 'eval_recall': 0.09302325581395349, 'eval_f1': 0.06639004149377593, 'eval_accuracy': 0.48291571753986334, 'eval_runtime': 0.5473, 'eval_samples_per_second': 91.364, 'eval_steps_per_second': 7.309, 'epoch': 0.3}
{'loss': 0.0577, 'learning_rate': 1.9108893245410802e-05, 'epoch': 0.45}
              precision    recall  f1-score   support

       NSWER       0.11      0.23      0.15        40
     UESTION       0.02      0.04      0.03        46

   micro avg       0.07      0.13      0.09        86
   macro avg       0.07      0.13      0.09        86
weighted avg       0.06      0.13      0.08        86

{'eval_loss': 6.321476459503174, 'eval_precision': 0.0658682634730539, 'eval_recall': 0.12790697674418605, 'eval_f1': 0.08695652173913043, 'eval_accuracy': 0.48721842571500884, 'eval_runtime': 1.6001, 'eval_samples_per_second': 31.248, 'eval_steps_per_second': 2.5, 'epoch': 0.45}
{'loss': 0.0563, 'learning_rate': 1.8811857660547735e-05, 'epoch': 0.59}
              precision    recall  f1-score   support

       NSWER       0.10      0.23      0.14        40
     UESTION       0.01      0.02      0.02        46

   micro avg       0.06      0.12      0.08        86
   macro avg       0.06      0.12      0.08        86
weighted avg       0.05      0.12      0.07        86

{'eval_loss': 6.650416374206543, 'eval_precision': 0.05747126436781609, 'eval_recall': 0.11627906976744186, 'eval_f1': 0.07692307692307693, 'eval_accuracy': 0.48316881802075423, 'eval_runtime': 0.5376, 'eval_samples_per_second': 93.01, 'eval_steps_per_second': 7.441, 'epoch': 0.59}
{'loss': 0.0527, 'learning_rate': 1.851482207568467e-05, 'epoch': 0.74}
              precision    recall  f1-score   support

       NSWER       0.11      0.17      0.14        40
     UESTION       0.03      0.04      0.04        46

   micro avg       0.07      0.10      0.09        86
   macro avg       0.07      0.11      0.09        86
weighted avg       0.07      0.10      0.08        86

{'eval_loss': 7.4319868087768555, 'eval_precision': 0.07258064516129033, 'eval_recall': 0.10465116279069768, 'eval_f1': 0.08571428571428572, 'eval_accuracy': 0.4650721336370539, 'eval_runtime': 0.5335, 'eval_samples_per_second': 93.714, 'eval_steps_per_second': 7.497, 'epoch': 0.74}
{'loss': 0.0493, 'learning_rate': 1.8217786490821602e-05, 'epoch': 0.89}
              precision    recall  f1-score   support

       NSWER       0.13      0.17      0.15        40
     UESTION       0.04      0.04      0.04        46

   micro avg       0.08      0.10      0.09        86
   macro avg       0.08      0.11      0.09        86
weighted avg       0.08      0.10      0.09        86

{'eval_loss': 8.248065948486328, 'eval_precision': 0.08181818181818182, 'eval_recall': 0.10465116279069768, 'eval_f1': 0.09183673469387756, 'eval_accuracy': 0.47886610984560873, 'eval_runtime': 0.5345, 'eval_samples_per_second': 93.543, 'eval_steps_per_second': 7.483, 'epoch': 0.89}
{'train_runtime': 2590.416, 'train_samples_per_second': 519.832, 'train_steps_per_second': 64.982, 'train_loss': 0.06484492696126302, 'epoch': 0.89}
              precision    recall  f1-score   support

       NSWER       0.09      0.17      0.12        59
     UESTION       0.06      0.10      0.07        73

   micro avg       0.07      0.13      0.09       132
   macro avg       0.07      0.13      0.09       132
weighted avg       0.07      0.13      0.09       132

{'epoch': 0.89,
 'eval_accuracy': 0.4448948807391345,
 'eval_f1': 0.0934065934065934,
 'eval_loss': 4.465493202209473,
 'eval_precision': 0.07327586206896551,
 'eval_recall': 0.12878787878787878,
 'eval_runtime': 1.8034,
 'eval_samples_per_second': 37.706,
 'eval_steps_per_second': 2.772}
Accuracy: 0.4448948807391345
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	2e-5	num_train_epochs	10	END 4744817: Thu 12 Oct 2023 12:02:30 PM EEST
