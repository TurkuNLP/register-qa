START 4753137: Fri 13 Oct 2023 12:18:34 PM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/dataset_punct2'], test='../../data/qa_token_classification/annotated/en_test_dataset.jsonl', dev='../../data/qa_token_classification/annotated/en_dev_dataset.jsonl', batch=8, epochs=10, lr=2e-05, save=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 134658
    })
    validation: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 1683
    })
    test: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 5693
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 134658
    })
})
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 134658
    })
    validation: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 40
    })
    test: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 60
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'loss': 0.1234, 'learning_rate': 1.9702964415136935e-05, 'epoch': 0.15}
              precision    recall  f1-score   support

       NSWER       0.04      0.10      0.06        40
     UESTION       0.04      0.04      0.04        92

   micro avg       0.04      0.06      0.05       132
   macro avg       0.04      0.07      0.05       132
weighted avg       0.04      0.06      0.05       132

{'eval_loss': 6.700128078460693, 'eval_precision': 0.043478260869565216, 'eval_recall': 0.06060606060606061, 'eval_f1': 0.05063291139240506, 'eval_accuracy': 0.303760162601626, 'eval_runtime': 0.6147, 'eval_samples_per_second': 65.077, 'eval_steps_per_second': 4.881, 'epoch': 0.15}
{'loss': 0.0684, 'learning_rate': 1.940592883027387e-05, 'epoch': 0.3}
              precision    recall  f1-score   support

       NSWER       0.02      0.05      0.03        40
     UESTION       0.03      0.03      0.03        92

   micro avg       0.03      0.04      0.03       132
   macro avg       0.03      0.04      0.03       132
weighted avg       0.03      0.04      0.03       132

{'eval_loss': 7.203049659729004, 'eval_precision': 0.026737967914438502, 'eval_recall': 0.03787878787878788, 'eval_f1': 0.031347962382445145, 'eval_accuracy': 0.3483739837398374, 'eval_runtime': 0.5705, 'eval_samples_per_second': 70.117, 'eval_steps_per_second': 5.259, 'epoch': 0.3}
{'loss': 0.0587, 'learning_rate': 1.9108893245410802e-05, 'epoch': 0.45}
              precision    recall  f1-score   support

       NSWER       0.04      0.10      0.06        40
     UESTION       0.04      0.04      0.04        92

   micro avg       0.04      0.06      0.05       132
   macro avg       0.04      0.07      0.05       132
weighted avg       0.04      0.06      0.05       132

{'eval_loss': 8.031444549560547, 'eval_precision': 0.04020100502512563, 'eval_recall': 0.06060606060606061, 'eval_f1': 0.04833836858006042, 'eval_accuracy': 0.3638211382113821, 'eval_runtime': 0.5789, 'eval_samples_per_second': 69.099, 'eval_steps_per_second': 5.182, 'epoch': 0.45}
{'loss': 0.0577, 'learning_rate': 1.8811857660547735e-05, 'epoch': 0.59}
              precision    recall  f1-score   support

       NSWER       0.04      0.10      0.06        40
     UESTION       0.03      0.03      0.03        92

   micro avg       0.04      0.05      0.04       132
   macro avg       0.04      0.07      0.05       132
weighted avg       0.03      0.05      0.04       132

{'eval_loss': 8.731588363647461, 'eval_precision': 0.03664921465968586, 'eval_recall': 0.05303030303030303, 'eval_f1': 0.043343653250773995, 'eval_accuracy': 0.3532520325203252, 'eval_runtime': 0.6267, 'eval_samples_per_second': 63.822, 'eval_steps_per_second': 4.787, 'epoch': 0.59}
{'loss': 0.0513, 'learning_rate': 1.851482207568467e-05, 'epoch': 0.74}
              precision    recall  f1-score   support

       NSWER       0.02      0.05      0.03        40
     UESTION       0.02      0.02      0.02        92

   micro avg       0.02      0.03      0.03       132
   macro avg       0.02      0.04      0.03       132
weighted avg       0.02      0.03      0.03       132

{'eval_loss': 9.992441177368164, 'eval_precision': 0.022857142857142857, 'eval_recall': 0.030303030303030304, 'eval_f1': 0.026058631921824105, 'eval_accuracy': 0.33191056910569106, 'eval_runtime': 0.572, 'eval_samples_per_second': 69.929, 'eval_steps_per_second': 5.245, 'epoch': 0.74}
{'loss': 0.0513, 'learning_rate': 1.8217786490821602e-05, 'epoch': 0.89}
              precision    recall  f1-score   support

       NSWER       0.01      0.03      0.02        40
     UESTION       0.01      0.01      0.01        92

   micro avg       0.01      0.02      0.01       132
   macro avg       0.01      0.02      0.01       132
weighted avg       0.01      0.02      0.01       132

{'eval_loss': 10.744229316711426, 'eval_precision': 0.011627906976744186, 'eval_recall': 0.015151515151515152, 'eval_f1': 0.013157894736842106, 'eval_accuracy': 0.32652439024390245, 'eval_runtime': 0.5732, 'eval_samples_per_second': 69.787, 'eval_steps_per_second': 5.234, 'epoch': 0.89}
{'train_runtime': 2694.3999, 'train_samples_per_second': 499.77, 'train_steps_per_second': 62.474, 'train_loss': 0.06845062967936198, 'epoch': 0.89}
              precision    recall  f1-score   support

       NSWER       0.05      0.06      0.05        79
     UESTION       0.00      0.00      0.00       109

   micro avg       0.02      0.03      0.02       188
   macro avg       0.02      0.03      0.03       188
weighted avg       0.02      0.03      0.02       188

{'epoch': 0.89,
 'eval_accuracy': 0.2273270949131077,
 'eval_f1': 0.024630541871921187,
 'eval_loss': 6.88266658782959,
 'eval_precision': 0.022935779816513763,
 'eval_recall': 0.026595744680851064,
 'eval_runtime': 0.8556,
 'eval_samples_per_second': 70.127,
 'eval_steps_per_second': 4.675}
Accuracy: 0.2273270949131077
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	2e-5	num_train_epochs	10	END 4753137: Fri 13 Oct 2023 01:08:43 PM EEST
