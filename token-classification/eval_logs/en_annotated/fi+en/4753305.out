START 4753305: Fri 13 Oct 2023 12:38:35 PM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/annotated/cleaned2_train_annotated_dataset.jsonl'], test='../../data/qa_token_classification/annotated/en_test_dataset.jsonl', dev='../../data/qa_token_classification/annotated/en_dev_dataset.jsonl', batch=8, epochs=10, lr=3e-05, save=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
    validation: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 40
    })
    test: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 60
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'train_runtime': 23.9464, 'train_samples_per_second': 41.76, 'train_steps_per_second': 5.429, 'train_loss': 0.4492081275353065, 'epoch': 10.0}
              precision    recall  f1-score   support

       NSWER       0.08      0.29      0.13        79
     UESTION       0.27      0.39      0.32       109

   micro avg       0.15      0.35      0.21       188
   macro avg       0.18      0.34      0.23       188
weighted avg       0.19      0.35      0.24       188

{'epoch': 10.0,
 'eval_accuracy': 0.7545205093928093,
 'eval_f1': 0.21324717285945072,
 'eval_loss': 0.6542491912841797,
 'eval_precision': 0.1531322505800464,
 'eval_recall': 0.35106382978723405,
 'eval_runtime': 0.9793,
 'eval_samples_per_second': 61.267,
 'eval_steps_per_second': 4.084}
Accuracy: 0.7545205093928093
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	3e-5	num_train_epochs	10	END 4753305: Fri 13 Oct 2023 12:39:38 PM EEST
