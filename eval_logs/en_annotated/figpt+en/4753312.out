START 4753312: Fri 13 Oct 2023 12:38:35 PM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/annotated/cleaned2_train_annotated_dataset.jsonl', '../../data/qa_token_classification/annotated/cleaned2_chatgpt_annotations_dataset.jsonl'], test='../../data/qa_token_classification/annotated/en_test_dataset.jsonl', dev='../../data/qa_token_classification/annotated/en_dev_dataset.jsonl', batch=8, epochs=10, lr=2e-05, save=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
in dictionary: 1
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 3424
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 3524
    })
})
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 3524
    })
    validation: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 40
    })
    test: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 60
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'loss': 0.8282, 'learning_rate': 1.886621315192744e-05, 'epoch': 0.57}
              precision    recall  f1-score   support

       NSWER       0.10      0.38      0.15        40
     UESTION       0.24      0.30      0.27        92

   micro avg       0.16      0.33      0.21       132
   macro avg       0.17      0.34      0.21       132
weighted avg       0.20      0.33      0.23       132

{'eval_loss': 0.535256028175354, 'eval_precision': 0.15925925925925927, 'eval_recall': 0.32575757575757575, 'eval_f1': 0.2139303482587065, 'eval_accuracy': 0.8123983739837398, 'eval_runtime': 0.7048, 'eval_samples_per_second': 56.758, 'eval_steps_per_second': 4.257, 'epoch': 0.57}
{'loss': 0.6534, 'learning_rate': 1.7732426303854877e-05, 'epoch': 1.13}
              precision    recall  f1-score   support

       NSWER       0.12      0.40      0.18        40
     UESTION       0.30      0.28      0.29        92

   micro avg       0.19      0.32      0.24       132
   macro avg       0.21      0.34      0.24       132
weighted avg       0.24      0.32      0.26       132

{'eval_loss': 0.4806137979030609, 'eval_precision': 0.18834080717488788, 'eval_recall': 0.3181818181818182, 'eval_f1': 0.23661971830985912, 'eval_accuracy': 0.7986788617886179, 'eval_runtime': 0.8521, 'eval_samples_per_second': 46.942, 'eval_steps_per_second': 3.521, 'epoch': 1.13}
{'loss': 0.5701, 'learning_rate': 1.6598639455782314e-05, 'epoch': 1.7}
              precision    recall  f1-score   support

       NSWER       0.11      0.25      0.16        40
     UESTION       0.28      0.24      0.26        92

   micro avg       0.19      0.24      0.22       132
   macro avg       0.20      0.24      0.21       132
weighted avg       0.23      0.24      0.23       132

{'eval_loss': 0.6025266647338867, 'eval_precision': 0.19393939393939394, 'eval_recall': 0.24242424242424243, 'eval_f1': 0.21548821548821545, 'eval_accuracy': 0.7513211382113821, 'eval_runtime': 0.5205, 'eval_samples_per_second': 76.849, 'eval_steps_per_second': 5.764, 'epoch': 1.7}
{'loss': 0.488, 'learning_rate': 1.546485260770975e-05, 'epoch': 2.27}
              precision    recall  f1-score   support

       NSWER       0.07      0.15      0.09        40
     UESTION       0.12      0.16      0.14        92

   micro avg       0.10      0.16      0.12       132
   macro avg       0.09      0.16      0.12       132
weighted avg       0.10      0.16      0.12       132

{'eval_loss': 0.6198530793190002, 'eval_precision': 0.09722222222222222, 'eval_recall': 0.1590909090909091, 'eval_f1': 0.1206896551724138, 'eval_accuracy': 0.7477642276422765, 'eval_runtime': 0.7267, 'eval_samples_per_second': 55.044, 'eval_steps_per_second': 4.128, 'epoch': 2.27}
{'loss': 0.4723, 'learning_rate': 1.433106575963719e-05, 'epoch': 2.83}
              precision    recall  f1-score   support

       NSWER       0.17      0.35      0.23        40
     UESTION       0.39      0.35      0.37        92

   micro avg       0.28      0.35      0.31       132
   macro avg       0.28      0.35      0.30       132
weighted avg       0.32      0.35      0.33       132

{'eval_loss': 0.5081155300140381, 'eval_precision': 0.2822085889570552, 'eval_recall': 0.3484848484848485, 'eval_f1': 0.311864406779661, 'eval_accuracy': 0.8209349593495935, 'eval_runtime': 0.5366, 'eval_samples_per_second': 74.537, 'eval_steps_per_second': 5.59, 'epoch': 2.83}
{'loss': 0.3809, 'learning_rate': 1.3197278911564626e-05, 'epoch': 3.4}
              precision    recall  f1-score   support

       NSWER       0.11      0.33      0.16        40
     UESTION       0.25      0.27      0.26        92

   micro avg       0.17      0.29      0.22       132
   macro avg       0.18      0.30      0.21       132
weighted avg       0.21      0.29      0.23       132

{'eval_loss': 0.6697272062301636, 'eval_precision': 0.17194570135746606, 'eval_recall': 0.2878787878787879, 'eval_f1': 0.21529745042492918, 'eval_accuracy': 0.7933943089430894, 'eval_runtime': 0.5136, 'eval_samples_per_second': 77.875, 'eval_steps_per_second': 5.841, 'epoch': 3.4}
{'loss': 0.3594, 'learning_rate': 1.2063492063492064e-05, 'epoch': 3.97}
              precision    recall  f1-score   support

       NSWER       0.12      0.33      0.18        40
     UESTION       0.21      0.26      0.23        92

   micro avg       0.17      0.28      0.21       132
   macro avg       0.16      0.29      0.20       132
weighted avg       0.18      0.28      0.21       132

{'eval_loss': 0.6460006237030029, 'eval_precision': 0.16517857142857142, 'eval_recall': 0.2803030303030303, 'eval_f1': 0.20786516853932582, 'eval_accuracy': 0.7426829268292683, 'eval_runtime': 0.5099, 'eval_samples_per_second': 78.444, 'eval_steps_per_second': 5.883, 'epoch': 3.97}
{'train_runtime': 338.7684, 'train_samples_per_second': 104.024, 'train_steps_per_second': 13.018, 'train_loss': 0.5360410853794643, 'epoch': 3.97}
              precision    recall  f1-score   support

       NSWER       0.22      0.43      0.29        79
     UESTION       0.45      0.49      0.47       109

   micro avg       0.32      0.46      0.38       188
   macro avg       0.34      0.46      0.38       188
weighted avg       0.35      0.46      0.39       188

{'epoch': 3.97,
 'eval_accuracy': 0.8895377471329065,
 'eval_f1': 0.3790849673202614,
 'eval_loss': 0.32651156187057495,
 'eval_precision': 0.3210332103321033,
 'eval_recall': 0.4627659574468085,
 'eval_runtime': 0.8153,
 'eval_samples_per_second': 73.594,
 'eval_steps_per_second': 4.906}
Accuracy: 0.8895377471329065
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	2e-5	num_train_epochs	10	END 4753312: Fri 13 Oct 2023 12:44:53 PM EEST
