START 4744785: Thu 12 Oct 2023 11:09:51 AM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/annotated/train_annotated_dataset.jsonl'], test='../../data/qa_token_classification/annotated/test_annotated_dataset.jsonl', dev='../../data/qa_token_classification/annotated/dev_annotated_dataset.jsonl', batch=8, epochs=10, lr=3e-05, save=None, dataset=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
    validation: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 50
    })
    test: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 68
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'loss': 1.0776, 'learning_rate': 2.423076923076923e-05, 'epoch': 1.92}
              precision    recall  f1-score   support

       NSWER       0.00      0.00      0.00        40
     UESTION       0.00      0.00      0.00        46

   micro avg       0.00      0.00      0.00        86
   macro avg       0.00      0.00      0.00        86
weighted avg       0.00      0.00      0.00        86

{'eval_loss': 1.002933382987976, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.4930397367754999, 'eval_runtime': 0.5695, 'eval_samples_per_second': 87.791, 'eval_steps_per_second': 7.023, 'epoch': 1.92}
{'loss': 0.907, 'learning_rate': 1.8461538461538465e-05, 'epoch': 3.85}
              precision    recall  f1-score   support

       NSWER       0.00      0.00      0.00        40
     UESTION       0.01      0.04      0.01        46

   micro avg       0.00      0.02      0.01        86
   macro avg       0.00      0.02      0.01        86
weighted avg       0.00      0.02      0.01        86

{'eval_loss': 0.8123054504394531, 'eval_precision': 0.002886002886002886, 'eval_recall': 0.023255813953488372, 'eval_f1': 0.005134788189987163, 'eval_accuracy': 0.6192103264996204, 'eval_runtime': 0.6309, 'eval_samples_per_second': 79.257, 'eval_steps_per_second': 6.341, 'epoch': 3.85}
{'loss': 0.5158, 'learning_rate': 1.2692307692307693e-05, 'epoch': 5.77}
              precision    recall  f1-score   support

       NSWER       0.06      0.30      0.10        40
     UESTION       0.06      0.20      0.09        46

   micro avg       0.06      0.24      0.10        86
   macro avg       0.06      0.25      0.10        86
weighted avg       0.06      0.24      0.10        86

{'eval_loss': 0.8150075674057007, 'eval_precision': 0.0603448275862069, 'eval_recall': 0.2441860465116279, 'eval_f1': 0.0967741935483871, 'eval_accuracy': 0.6641356618577575, 'eval_runtime': 0.5941, 'eval_samples_per_second': 84.165, 'eval_steps_per_second': 6.733, 'epoch': 5.77}
{'loss': 0.2459, 'learning_rate': 6.923076923076923e-06, 'epoch': 7.69}
              precision    recall  f1-score   support

       NSWER       0.06      0.28      0.09        40
     UESTION       0.11      0.28      0.16        46

   micro avg       0.08      0.28      0.12        86
   macro avg       0.08      0.28      0.13        86
weighted avg       0.09      0.28      0.13        86

{'eval_loss': 0.7217726111412048, 'eval_precision': 0.07667731629392971, 'eval_recall': 0.27906976744186046, 'eval_f1': 0.12030075187969924, 'eval_accuracy': 0.7408251075677044, 'eval_runtime': 0.7435, 'eval_samples_per_second': 67.249, 'eval_steps_per_second': 5.38, 'epoch': 7.69}
{'loss': 0.1132, 'learning_rate': 1.153846153846154e-06, 'epoch': 9.62}
              precision    recall  f1-score   support

       NSWER       0.09      0.42      0.15        40
     UESTION       0.18      0.43      0.26        46

   micro avg       0.13      0.43      0.20        86
   macro avg       0.14      0.43      0.21        86
weighted avg       0.14      0.43      0.21        86

{'eval_loss': 0.8955883979797363, 'eval_precision': 0.12758620689655173, 'eval_recall': 0.43023255813953487, 'eval_f1': 0.19680851063829788, 'eval_accuracy': 0.7009617818273854, 'eval_runtime': 0.7217, 'eval_samples_per_second': 69.281, 'eval_steps_per_second': 5.542, 'epoch': 9.62}
{'train_runtime': 45.0273, 'train_samples_per_second': 22.209, 'train_steps_per_second': 2.887, 'train_loss': 0.5517483903811529, 'epoch': 10.0}
              precision    recall  f1-score   support

       NSWER       0.08      0.34      0.13        59
     UESTION       0.15      0.41      0.22        73

   micro avg       0.11      0.38      0.18       132
   macro avg       0.12      0.37      0.18       132
weighted avg       0.12      0.38      0.18       132

{'epoch': 10.0,
 'eval_accuracy': 0.7086829452248515,
 'eval_f1': 0.17543859649122806,
 'eval_loss': 0.7603027820587158,
 'eval_precision': 0.1141552511415525,
 'eval_recall': 0.3787878787878788,
 'eval_runtime': 0.7377,
 'eval_samples_per_second': 92.176,
 'eval_steps_per_second': 6.778}
Accuracy: 0.7086829452248515
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	3e-5	num_train_epochs	10	END 4744785: Thu 12 Oct 2023 11:10:56 AM EEST
