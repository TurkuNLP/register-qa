START 4744750: Thu 12 Oct 2023 11:04:28 AM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/annotated/train_annotated_dataset.jsonl'], test='../../data/qa_token_classification/dataset_punct2', dev='../../data/qa_token_classification/dataset_punct2', batch=8, epochs=10, lr=2e-05, save=None, dataset=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
    validation: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 1683
    })
    test: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 5693
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'train_runtime': 22.7543, 'train_samples_per_second': 43.948, 'train_steps_per_second': 5.713, 'train_loss': 0.5253595792330228, 'epoch': 10.0}
              precision    recall  f1-score   support

       NSWER       0.04      0.24      0.06      5623
     UESTION       0.01      0.06      0.02      5693

   micro avg       0.03      0.15      0.05     11316
   macro avg       0.02      0.15      0.04     11316
weighted avg       0.02      0.15      0.04     11316

{'epoch': 10.0,
 'eval_accuracy': 0.7039000491415127,
 'eval_f1': 0.045058730976860265,
 'eval_loss': 0.8272607922554016,
 'eval_precision': 0.026614324627463548,
 'eval_recall': 0.14678331565924355,
 'eval_runtime': 80.3298,
 'eval_samples_per_second': 70.87,
 'eval_steps_per_second': 4.432}
Accuracy: 0.7039000491415127
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	2e-5	num_train_epochs	10	END 4744750: Thu 12 Oct 2023 11:06:47 AM EEST
