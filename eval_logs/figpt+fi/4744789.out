START 4744789: Thu 12 Oct 2023 11:10:31 AM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/annotated/train_annotated_dataset.jsonl', '../../data/qa_token_classification/annotated/cleaned2_chatgpt_annotations_dataset.jsonl'], test='../../data/qa_token_classification/annotated/test_annotated_dataset.jsonl', dev='../../data/qa_token_classification/annotated/dev_annotated_dataset.jsonl', batch=8, epochs=10, lr=5e-05, save=None, dataset=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
in dictionary: 1
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 3424
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 3524
    })
})
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 3524
    })
    validation: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 50
    })
    test: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 68
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'loss': 0.8533, 'learning_rate': 4.71655328798186e-05, 'epoch': 0.57}
              precision    recall  f1-score   support

       NSWER       0.08      0.23      0.12        40
     UESTION       0.17      0.48      0.25        46

   micro avg       0.13      0.36      0.19        86
   macro avg       0.13      0.35      0.18        86
weighted avg       0.13      0.36      0.19        86

{'eval_loss': 0.6469030976295471, 'eval_precision': 0.12863070539419086, 'eval_recall': 0.36046511627906974, 'eval_f1': 0.18960244648318042, 'eval_accuracy': 0.7482915717539863, 'eval_runtime': 0.5239, 'eval_samples_per_second': 95.43, 'eval_steps_per_second': 7.634, 'epoch': 0.57}
{'loss': 0.6831, 'learning_rate': 4.433106575963719e-05, 'epoch': 1.13}
              precision    recall  f1-score   support

       NSWER       0.17      0.45      0.25        40
     UESTION       0.18      0.43      0.25        46

   micro avg       0.17      0.44      0.25        86
   macro avg       0.17      0.44      0.25        86
weighted avg       0.17      0.44      0.25        86

{'eval_loss': 0.487804651260376, 'eval_precision': 0.1743119266055046, 'eval_recall': 0.4418604651162791, 'eval_f1': 0.25000000000000006, 'eval_accuracy': 0.7803087825866869, 'eval_runtime': 0.5431, 'eval_samples_per_second': 92.057, 'eval_steps_per_second': 7.365, 'epoch': 1.13}
{'loss': 0.6351, 'learning_rate': 4.149659863945579e-05, 'epoch': 1.7}
              precision    recall  f1-score   support

       NSWER       0.24      0.33      0.28        40
     UESTION       0.15      0.30      0.20        46

   micro avg       0.18      0.31      0.23        86
   macro avg       0.20      0.31      0.24        86
weighted avg       0.19      0.31      0.24        86

{'eval_loss': 0.5704023241996765, 'eval_precision': 0.1836734693877551, 'eval_recall': 0.313953488372093, 'eval_f1': 0.23175965665236054, 'eval_accuracy': 0.6983042267780308, 'eval_runtime': 0.5358, 'eval_samples_per_second': 93.318, 'eval_steps_per_second': 7.465, 'epoch': 1.7}
{'loss': 0.5413, 'learning_rate': 3.8662131519274384e-05, 'epoch': 2.27}
              precision    recall  f1-score   support

       NSWER       0.15      0.28      0.19        40
     UESTION       0.22      0.22      0.22        46

   micro avg       0.17      0.24      0.20        86
   macro avg       0.18      0.25      0.20        86
weighted avg       0.18      0.24      0.21        86

{'eval_loss': 0.7554475665092468, 'eval_precision': 0.17355371900826447, 'eval_recall': 0.2441860465116279, 'eval_f1': 0.2028985507246377, 'eval_accuracy': 0.675904834219185, 'eval_runtime': 1.5563, 'eval_samples_per_second': 32.128, 'eval_steps_per_second': 2.57, 'epoch': 2.27}
{'loss': 0.4812, 'learning_rate': 3.5827664399092974e-05, 'epoch': 2.83}
              precision    recall  f1-score   support

       NSWER       0.18      0.42      0.26        40
     UESTION       0.25      0.54      0.34        46

   micro avg       0.22      0.49      0.30        86
   macro avg       0.21      0.48      0.30        86
weighted avg       0.22      0.49      0.30        86

{'eval_loss': 0.533189058303833, 'eval_precision': 0.21649484536082475, 'eval_recall': 0.4883720930232558, 'eval_f1': 0.3, 'eval_accuracy': 0.7927107061503417, 'eval_runtime': 0.5509, 'eval_samples_per_second': 90.761, 'eval_steps_per_second': 7.261, 'epoch': 2.83}
{'loss': 0.4215, 'learning_rate': 3.2993197278911564e-05, 'epoch': 3.4}
              precision    recall  f1-score   support

       NSWER       0.28      0.53      0.37        40
     UESTION       0.24      0.48      0.32        46

   micro avg       0.26      0.50      0.34        86
   macro avg       0.26      0.50      0.34        86
weighted avg       0.26      0.50      0.34        86

{'eval_loss': 0.664580225944519, 'eval_precision': 0.25748502994011974, 'eval_recall': 0.5, 'eval_f1': 0.33992094861660077, 'eval_accuracy': 0.7491774234371046, 'eval_runtime': 0.5481, 'eval_samples_per_second': 91.227, 'eval_steps_per_second': 7.298, 'epoch': 3.4}
{'loss': 0.3754, 'learning_rate': 3.0158730158730158e-05, 'epoch': 3.97}
              precision    recall  f1-score   support

       NSWER       0.25      0.45      0.32        40
     UESTION       0.21      0.39      0.27        46

   micro avg       0.23      0.42      0.29        86
   macro avg       0.23      0.42      0.30        86
weighted avg       0.23      0.42      0.29        86

{'eval_loss': 0.9373664855957031, 'eval_precision': 0.22641509433962265, 'eval_recall': 0.4186046511627907, 'eval_f1': 0.29387755102040813, 'eval_accuracy': 0.7217160212604404, 'eval_runtime': 0.5439, 'eval_samples_per_second': 91.921, 'eval_steps_per_second': 7.354, 'epoch': 3.97}
{'train_runtime': 328.1268, 'train_samples_per_second': 107.398, 'train_steps_per_second': 13.44, 'train_loss': 0.5701210109165736, 'epoch': 3.97}
              precision    recall  f1-score   support

       NSWER       0.22      0.47      0.30        59
     UESTION       0.26      0.62      0.36        73

   micro avg       0.24      0.55      0.34       132
   macro avg       0.24      0.55      0.33       132
weighted avg       0.24      0.55      0.34       132

{'epoch': 3.97,
 'eval_accuracy': 0.8524559253323277,
 'eval_f1': 0.33640552995391704,
 'eval_loss': 0.4042949676513672,
 'eval_precision': 0.24172185430463577,
 'eval_recall': 0.553030303030303,
 'eval_runtime': 0.7601,
 'eval_samples_per_second': 89.456,
 'eval_steps_per_second': 6.578}
Accuracy: 0.8524559253323277
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	5e-5	num_train_epochs	10	END 4744789: Thu 12 Oct 2023 11:16:38 AM EEST
