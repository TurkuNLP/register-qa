START 4744715: Thu 12 Oct 2023 11:01:15 AM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/dataset_punct2', '../../data/qa_token_classification/annotated/train_annotated_dataset.jsonl', '../../data/qa_token_classification/annotated/cleaned2_chatgpt_annotations_dataset.jsonl'], test='../../data/qa_token_classification/dataset_punct2', dev='../../data/qa_token_classification/dataset_punct2', batch=8, epochs=10, lr=2e-05, save=None, dataset=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 134658
    })
    validation: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 1683
    })
    test: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 5693
    })
})
in dictionary: 1
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
in dictionary: 2
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 3424
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens', 'text'],
        num_rows: 138182
    })
})
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens', 'text'],
        num_rows: 138182
    })
    validation: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 1683
    })
    test: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 5693
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'loss': 0.1457, 'learning_rate': 1.9710530886354428e-05, 'epoch': 0.14}
              precision    recall  f1-score   support

       NSWER       0.64      0.78      0.70      1659
     UESTION       0.64      0.79      0.71      1683

   micro avg       0.64      0.79      0.70      3342
   macro avg       0.64      0.79      0.70      3342
weighted avg       0.64      0.79      0.70      3342

{'eval_loss': 0.11876504868268967, 'eval_precision': 0.6384634087041089, 'eval_recall': 0.7857570317175344, 'eval_f1': 0.7044936284372905, 'eval_accuracy': 0.9681784901501075, 'eval_runtime': 24.7292, 'eval_samples_per_second': 68.057, 'eval_steps_per_second': 4.286, 'epoch': 0.14}
{'loss': 0.0921, 'learning_rate': 1.9421061772708855e-05, 'epoch': 0.29}
              precision    recall  f1-score   support

       NSWER       0.76      0.87      0.81      1659
     UESTION       0.76      0.86      0.81      1683

   micro avg       0.76      0.86      0.81      3342
   macro avg       0.76      0.86      0.81      3342
weighted avg       0.76      0.86      0.81      3342

{'eval_loss': 0.07907193154096603, 'eval_precision': 0.7618670886075949, 'eval_recall': 0.8644524236983842, 'eval_f1': 0.8099243061396131, 'eval_accuracy': 0.9730922028181821, 'eval_runtime': 24.518, 'eval_samples_per_second': 68.644, 'eval_steps_per_second': 4.323, 'epoch': 0.29}
{'loss': 0.0754, 'learning_rate': 1.913159265906328e-05, 'epoch': 0.43}
              precision    recall  f1-score   support

       NSWER       0.80      0.87      0.83      1659
     UESTION       0.80      0.86      0.83      1683

   micro avg       0.80      0.87      0.83      3342
   macro avg       0.80      0.87      0.83      3342
weighted avg       0.80      0.87      0.83      3342

{'eval_loss': 0.11455752700567245, 'eval_precision': 0.7992277992277992, 'eval_recall': 0.8671454219030521, 'eval_f1': 0.8318025258323766, 'eval_accuracy': 0.9748922234351582, 'eval_runtime': 24.3575, 'eval_samples_per_second': 69.096, 'eval_steps_per_second': 4.352, 'epoch': 0.43}
{'loss': 0.0751, 'learning_rate': 1.8842123545417705e-05, 'epoch': 0.58}
              precision    recall  f1-score   support

       NSWER       0.82      0.89      0.85      1659
     UESTION       0.82      0.88      0.85      1683

   micro avg       0.82      0.89      0.85      3342
   macro avg       0.82      0.89      0.85      3342
weighted avg       0.82      0.89      0.85      3342

{'eval_loss': 0.07968847453594208, 'eval_precision': 0.82176568573015, 'eval_recall': 0.8856971873129862, 'eval_f1': 0.8525345622119817, 'eval_accuracy': 0.9743133621794787, 'eval_runtime': 24.5355, 'eval_samples_per_second': 68.595, 'eval_steps_per_second': 4.32, 'epoch': 0.58}
{'loss': 0.0696, 'learning_rate': 1.855265443177213e-05, 'epoch': 0.72}
              precision    recall  f1-score   support

       NSWER       0.85      0.89      0.87      1659
     UESTION       0.85      0.89      0.87      1683

   micro avg       0.85      0.89      0.87      3342
   macro avg       0.85      0.89      0.87      3342
weighted avg       0.85      0.89      0.87      3342

{'eval_loss': 0.09125614166259766, 'eval_precision': 0.8454106280193237, 'eval_recall': 0.8901855176540994, 'eval_f1': 0.8672205217898266, 'eval_accuracy': 0.9793248733245403, 'eval_runtime': 24.6784, 'eval_samples_per_second': 68.197, 'eval_steps_per_second': 4.295, 'epoch': 0.72}
{'loss': 0.0617, 'learning_rate': 1.8263185318126558e-05, 'epoch': 0.87}
              precision    recall  f1-score   support

       NSWER       0.83      0.89      0.86      1659
     UESTION       0.82      0.89      0.85      1683

   micro avg       0.83      0.89      0.86      3342
   macro avg       0.83      0.89      0.86      3342
weighted avg       0.83      0.89      0.86      3342

{'eval_loss': 0.14928001165390015, 'eval_precision': 0.825083426028921, 'eval_recall': 0.8877917414721723, 'eval_f1': 0.8552897088498126, 'eval_accuracy': 0.9727644457601717, 'eval_runtime': 24.6207, 'eval_samples_per_second': 68.357, 'eval_steps_per_second': 4.305, 'epoch': 0.87}
{'loss': 0.065, 'learning_rate': 1.7973716204480985e-05, 'epoch': 1.01}
              precision    recall  f1-score   support

       NSWER       0.86      0.90      0.88      1659
     UESTION       0.86      0.90      0.88      1683

   micro avg       0.86      0.90      0.88      3342
   macro avg       0.86      0.90      0.88      3342
weighted avg       0.86      0.90      0.88      3342

{'eval_loss': 0.10380726307630539, 'eval_precision': 0.8631669535283993, 'eval_recall': 0.900359066427289, 'eval_f1': 0.8813708260105448, 'eval_accuracy': 0.9792059292309075, 'eval_runtime': 24.5273, 'eval_samples_per_second': 68.617, 'eval_steps_per_second': 4.322, 'epoch': 1.01}
{'train_runtime': 3210.6668, 'train_samples_per_second': 430.384, 'train_steps_per_second': 53.799, 'train_loss': 0.08351545758928572, 'epoch': 1.01}
              precision    recall  f1-score   support

       NSWER       0.77      0.87      0.82      5623
     UESTION       0.77      0.86      0.81      5693

   micro avg       0.77      0.86      0.81     11316
   macro avg       0.77      0.86      0.81     11316
weighted avg       0.77      0.86      0.81     11316

{'epoch': 1.01,
 'eval_accuracy': 0.9768807601180964,
 'eval_f1': 0.8129445530551975,
 'eval_loss': 0.06938883662223816,
 'eval_precision': 0.7679371316306484,
 'eval_recall': 0.8635560268646165,
 'eval_runtime': 84.7734,
 'eval_samples_per_second': 67.156,
 'eval_steps_per_second': 4.199}
Accuracy: 0.9768807601180964
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	2e-5	num_train_epochs	10	END 4744715: Thu 12 Oct 2023 12:02:27 PM EEST
