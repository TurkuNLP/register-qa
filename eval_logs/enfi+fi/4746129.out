START 4746129: Thu 12 Oct 2023 01:17:41 PM EEST
Namespace(model_name='xlm-roberta-base', train=['../../data/qa_token_classification/dataset_punct2', '../../data/qa_token_classification/annotated/train_annotated_dataset.jsonl'], test='../../data/qa_token_classification/annotated/test_annotated_dataset.jsonl', dev='../../data/qa_token_classification//annotated/dev_annotated_dataset.jsonl', batch=8, epochs=10, lr=5e-05, save=None, dataset=None)
in dictionary: 0
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 134658
    })
    validation: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 1683
    })
    test: Dataset({
        features: ['id', 'tags', 'tokens'],
        num_rows: 5693
    })
})
Downloading and preparing dataset json/default to /scratch/project_462000321/anni/cache/json/default-daececf74cb46ba9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...
Dataset json downloaded and prepared to /scratch/project_462000321/anni/cache/json/default-daececf74cb46ba9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.
in dictionary: 1
DatasetDict({
    train: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 100
    })
})
final train:
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens', 'text'],
        num_rows: 134758
    })
})
Downloading and preparing dataset json/default to /scratch/project_462000321/anni/cache/json/default-ef26603b715bf39d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...
Dataset json downloaded and prepared to /scratch/project_462000321/anni/cache/json/default-ef26603b715bf39d/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.
DatasetDict({
    train: Dataset({
        features: ['id', 'tags', 'tokens', 'text'],
        num_rows: 134758
    })
    validation: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 50
    })
    test: Dataset({
        features: ['tags', 'tokens', 'id', 'text'],
        num_rows: 68
    })
})
['QUESTION', 'ANSWER', 'O']
3
tokenization
training
{'loss': 0.2802, 'learning_rate': 4.992579400415554e-05, 'epoch': 0.01}
              precision    recall  f1-score   support

       NSWER       0.03      0.10      0.05        40
     UESTION       0.02      0.04      0.02        46

   micro avg       0.03      0.07      0.04        86
   macro avg       0.03      0.07      0.04        86
weighted avg       0.02      0.07      0.04        86

{'eval_loss': 3.300029993057251, 'eval_precision': 0.025, 'eval_recall': 0.06976744186046512, 'eval_f1': 0.03680981595092024, 'eval_accuracy': 0.4527967603138446, 'eval_runtime': 0.5757, 'eval_samples_per_second': 86.851, 'eval_steps_per_second': 6.948, 'epoch': 0.01}
{'loss': 0.1649, 'learning_rate': 4.9851588008311077e-05, 'epoch': 0.03}
              precision    recall  f1-score   support

       NSWER       0.04      0.10      0.05        40
     UESTION       0.03      0.07      0.04        46

   micro avg       0.03      0.08      0.05        86
   macro avg       0.03      0.08      0.05        86
weighted avg       0.03      0.08      0.04        86

{'eval_loss': 4.329596519470215, 'eval_precision': 0.03111111111111111, 'eval_recall': 0.08139534883720931, 'eval_f1': 0.045016077170418, 'eval_accuracy': 0.49025563148569984, 'eval_runtime': 0.5376, 'eval_samples_per_second': 93.001, 'eval_steps_per_second': 7.44, 'epoch': 0.03}
{'loss': 0.1482, 'learning_rate': 4.977738201246661e-05, 'epoch': 0.04}
              precision    recall  f1-score   support

       NSWER       0.06      0.15      0.09        40
     UESTION       0.02      0.04      0.03        46

   micro avg       0.04      0.09      0.06        86
   macro avg       0.04      0.10      0.06        86
weighted avg       0.04      0.09      0.06        86

{'eval_loss': 3.305807590484619, 'eval_precision': 0.041666666666666664, 'eval_recall': 0.09302325581395349, 'eval_f1': 0.05755395683453238, 'eval_accuracy': 0.47114654517843585, 'eval_runtime': 0.5357, 'eval_samples_per_second': 93.33, 'eval_steps_per_second': 7.466, 'epoch': 0.04}
{'loss': 0.1258, 'learning_rate': 4.9703176016622144e-05, 'epoch': 0.06}
              precision    recall  f1-score   support

       NSWER       0.06      0.12      0.08        40
     UESTION       0.01      0.02      0.02        46

   micro avg       0.04      0.07      0.05        86
   macro avg       0.04      0.07      0.05        86
weighted avg       0.04      0.07      0.05        86

{'eval_loss': 4.363715171813965, 'eval_precision': 0.037267080745341616, 'eval_recall': 0.06976744186046512, 'eval_f1': 0.048582995951417005, 'eval_accuracy': 0.4944317894203999, 'eval_runtime': 0.535, 'eval_samples_per_second': 93.466, 'eval_steps_per_second': 7.477, 'epoch': 0.06}
{'loss': 0.1004, 'learning_rate': 4.9628970020777684e-05, 'epoch': 0.07}
              precision    recall  f1-score   support

       NSWER       0.05      0.10      0.07        40
     UESTION       0.02      0.04      0.03        46

   micro avg       0.04      0.07      0.05        86
   macro avg       0.04      0.07      0.05        86
weighted avg       0.03      0.07      0.05        86

{'eval_loss': 4.3057637214660645, 'eval_precision': 0.03508771929824561, 'eval_recall': 0.06976744186046512, 'eval_f1': 0.046692607003891044, 'eval_accuracy': 0.5036699569729183, 'eval_runtime': 0.5365, 'eval_samples_per_second': 93.2, 'eval_steps_per_second': 7.456, 'epoch': 0.07}
{'loss': 0.0973, 'learning_rate': 4.955476402493322e-05, 'epoch': 0.09}
              precision    recall  f1-score   support

       NSWER       0.07      0.17      0.10        40
     UESTION       0.01      0.02      0.01        46

   micro avg       0.04      0.09      0.05        86
   macro avg       0.04      0.10      0.05        86
weighted avg       0.04      0.09      0.05        86

{'eval_loss': 4.299319267272949, 'eval_precision': 0.03773584905660377, 'eval_recall': 0.09302325581395349, 'eval_f1': 0.053691275167785234, 'eval_accuracy': 0.4635535307517084, 'eval_runtime': 0.5352, 'eval_samples_per_second': 93.432, 'eval_steps_per_second': 7.475, 'epoch': 0.09}
{'train_runtime': 281.6165, 'train_samples_per_second': 4785.16, 'train_steps_per_second': 598.154, 'train_loss': 0.15279345575968425, 'epoch': 0.09}
              precision    recall  f1-score   support

       NSWER       0.02      0.05      0.03        59
     UESTION       0.02      0.03      0.02        73

   micro avg       0.02      0.04      0.03       132
   macro avg       0.02      0.04      0.03       132
weighted avg       0.02      0.04      0.03       132

{'epoch': 0.09,
 'eval_accuracy': 0.46092203261996795,
 'eval_f1': 0.02666666666666667,
 'eval_loss': 3.3559367656707764,
 'eval_precision': 0.0205761316872428,
 'eval_recall': 0.03787878787878788,
 'eval_runtime': 0.7915,
 'eval_samples_per_second': 85.916,
 'eval_steps_per_second': 6.317}
Accuracy: 0.46092203261996795
PARAMETERS	model	xlm-roberta-base	data_dir	../../data/qa_token_classification	train_batch_size	8	learning_rate	5e-5	num_train_epochs	10	END 4746129: Thu 12 Oct 2023 01:27:21 PM EEST
