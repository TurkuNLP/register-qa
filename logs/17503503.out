START: Tue Jun 27 16:00:08 EEST 2023
Namespace(train_set=['data/CORE-corpus/train.tsv.gz', 'data/FinCORE_full/train.tsv', 'data/SweCORE/swe_train.tsv', 'data/FreCORE/fre_train.tsv'], dev_set=['data/CORE-corpus/dev.tsv.gz', 'data/FinCORE_full/dev.tsv', 'data/SweCORE/swe_dev.tsv', 'data/FreCORE/fre_dev.tsv'], test_set=['data/CORE-corpus/test.tsv.gz', 'data/FinCORE_full/test.tsv', 'data/SweCORE/swe_test.tsv', 'data/FreCORE/fre_test.tsv'], model='xlm-roberta-base', batch=8, epochs=10, learning=8e-06, save=True, save_name='register-qa-binary-8e-6', weights=True)
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 45597
    })
    dev: Dataset({
        features: ['text', 'label'],
        num_rows: 7609
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 14377
    })
})
sub-register mapping
filtering
train 44313
test 13915
dev 7349
change labels to only qa
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
tokenization
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[0 1]
[ 0.51464508 17.57057891]
2
training
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.8444, 'learning_rate': 7.927797833935018e-06, 'epoch': 0.09}
              precision    recall  f1-score   support

      NOT_QA       0.97      1.00      0.99      7154
      QA_NEW       0.00      0.00      0.00       195

    accuracy                           0.97      7349
   macro avg       0.49      0.50      0.49      7349
weighted avg       0.95      0.97      0.96      7349

{'eval_loss': 1.8026533126831055, 'eval_accuracy': 0.9734657776568241, 'eval_f1': 0.9734657776568241, 'eval_precision': 0.9734657776568241, 'eval_recall': 0.9734657776568241, 'eval_runtime': 89.1713, 'eval_samples_per_second': 82.414, 'eval_steps_per_second': 2.579, 'epoch': 0.09}
{'loss': 0.924, 'learning_rate': 7.855595667870035e-06, 'epoch': 0.18}
              precision    recall  f1-score   support

      NOT_QA       0.97      1.00      0.99      7154
      QA_NEW       0.00      0.00      0.00       195

    accuracy                           0.97      7349
   macro avg       0.49      0.50      0.49      7349
weighted avg       0.95      0.97      0.96      7349

{'eval_loss': 1.714801549911499, 'eval_accuracy': 0.9734657776568241, 'eval_f1': 0.9734657776568241, 'eval_precision': 0.9734657776568241, 'eval_recall': 0.9734657776568241, 'eval_runtime': 69.5265, 'eval_samples_per_second': 105.701, 'eval_steps_per_second': 3.308, 'epoch': 0.18}
{'loss': 0.7414, 'learning_rate': 7.783393501805054e-06, 'epoch': 0.27}
              precision    recall  f1-score   support

      NOT_QA       0.97      1.00      0.99      7154
      QA_NEW       0.00      0.00      0.00       195

    accuracy                           0.97      7349
   macro avg       0.49      0.50      0.49      7349
weighted avg       0.95      0.97      0.96      7349

{'eval_loss': 1.5245181322097778, 'eval_accuracy': 0.9734657776568241, 'eval_f1': 0.9734657776568241, 'eval_precision': 0.9734657776568241, 'eval_recall': 0.9734657776568241, 'eval_runtime': 69.5678, 'eval_samples_per_second': 105.638, 'eval_steps_per_second': 3.306, 'epoch': 0.27}
{'loss': 0.6604, 'learning_rate': 7.711191335740073e-06, 'epoch': 0.36}
              precision    recall  f1-score   support

      NOT_QA       0.97      1.00      0.99      7154
      QA_NEW       0.00      0.00      0.00       195

    accuracy                           0.97      7349
   macro avg       0.49      0.50      0.49      7349
weighted avg       0.95      0.97      0.96      7349

{'eval_loss': 1.6383370161056519, 'eval_accuracy': 0.9734657776568241, 'eval_f1': 0.9734657776568241, 'eval_precision': 0.9734657776568241, 'eval_recall': 0.9734657776568241, 'eval_runtime': 77.7087, 'eval_samples_per_second': 94.571, 'eval_steps_per_second': 2.96, 'epoch': 0.36}
{'loss': 0.5226, 'learning_rate': 7.63898916967509e-06, 'epoch': 0.45}
              precision    recall  f1-score   support

      NOT_QA       0.98      0.99      0.99      7154
      QA_NEW       0.62      0.36      0.46       195

    accuracy                           0.98      7349
   macro avg       0.80      0.68      0.72      7349
weighted avg       0.97      0.98      0.97      7349

{'eval_loss': 1.2070600986480713, 'eval_accuracy': 0.9772758198394339, 'eval_f1': 0.9772758198394339, 'eval_precision': 0.9772758198394339, 'eval_recall': 0.9772758198394339, 'eval_runtime': 69.6206, 'eval_samples_per_second': 105.558, 'eval_steps_per_second': 3.304, 'epoch': 0.45}
{'loss': 0.48, 'learning_rate': 7.566787003610108e-06, 'epoch': 0.54}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.88      0.34      0.49       195

    accuracy                           0.98      7349
   macro avg       0.93      0.67      0.74      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.3722950220108032, 'eval_accuracy': 0.981221934957137, 'eval_f1': 0.9812219349571372, 'eval_precision': 0.981221934957137, 'eval_recall': 0.981221934957137, 'eval_runtime': 69.7093, 'eval_samples_per_second': 105.423, 'eval_steps_per_second': 3.299, 'epoch': 0.54}
{'loss': 0.4778, 'learning_rate': 7.494584837545126e-06, 'epoch': 0.63}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.96      0.34      0.50       195

    accuracy                           0.98      7349
   macro avg       0.97      0.67      0.75      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.2334957122802734, 'eval_accuracy': 0.9820383725676963, 'eval_f1': 0.9820383725676963, 'eval_precision': 0.9820383725676963, 'eval_recall': 0.9820383725676963, 'eval_runtime': 69.7243, 'eval_samples_per_second': 105.401, 'eval_steps_per_second': 3.299, 'epoch': 0.63}
{'loss': 0.4988, 'learning_rate': 7.422382671480144e-06, 'epoch': 0.72}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.79      0.41      0.54       195

    accuracy                           0.98      7349
   macro avg       0.89      0.70      0.77      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.08320951461792, 'eval_accuracy': 0.9814940808273235, 'eval_f1': 0.9814940808273235, 'eval_precision': 0.9814940808273235, 'eval_recall': 0.9814940808273235, 'eval_runtime': 69.52, 'eval_samples_per_second': 105.711, 'eval_steps_per_second': 3.308, 'epoch': 0.72}
{'loss': 0.4925, 'learning_rate': 7.350180505415162e-06, 'epoch': 0.81}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.78      0.43      0.55       195

    accuracy                           0.98      7349
   macro avg       0.88      0.71      0.77      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 0.9465727806091309, 'eval_accuracy': 0.9816301537624167, 'eval_f1': 0.9816301537624167, 'eval_precision': 0.9816301537624167, 'eval_recall': 0.9816301537624167, 'eval_runtime': 69.4665, 'eval_samples_per_second': 105.792, 'eval_steps_per_second': 3.311, 'epoch': 0.81}
{'loss': 0.438, 'learning_rate': 7.27797833935018e-06, 'epoch': 0.9}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.86      0.41      0.55       195

    accuracy                           0.98      7349
   macro avg       0.92      0.70      0.77      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.094031810760498, 'eval_accuracy': 0.982446591372976, 'eval_f1': 0.982446591372976, 'eval_precision': 0.982446591372976, 'eval_recall': 0.982446591372976, 'eval_runtime': 69.5813, 'eval_samples_per_second': 105.617, 'eval_steps_per_second': 3.305, 'epoch': 0.9}
{'loss': 0.5056, 'learning_rate': 7.205776173285198e-06, 'epoch': 0.99}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.90      0.37      0.53       195

    accuracy                           0.98      7349
   macro avg       0.94      0.69      0.76      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 0.9042472839355469, 'eval_accuracy': 0.9823105184378828, 'eval_f1': 0.9823105184378828, 'eval_precision': 0.9823105184378828, 'eval_recall': 0.9823105184378828, 'eval_runtime': 69.8504, 'eval_samples_per_second': 105.211, 'eval_steps_per_second': 3.293, 'epoch': 0.99}
{'loss': 0.38, 'learning_rate': 7.133574007220217e-06, 'epoch': 1.08}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.82      0.38      0.52       195

    accuracy                           0.98      7349
   macro avg       0.90      0.69      0.76      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.0841549634933472, 'eval_accuracy': 0.9814940808273235, 'eval_f1': 0.9814940808273235, 'eval_precision': 0.9814940808273235, 'eval_recall': 0.9814940808273235, 'eval_runtime': 69.8984, 'eval_samples_per_second': 105.138, 'eval_steps_per_second': 3.29, 'epoch': 1.08}
{'loss': 0.426, 'learning_rate': 7.0613718411552345e-06, 'epoch': 1.17}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.87      0.43      0.57       195

    accuracy                           0.98      7349
   macro avg       0.93      0.71      0.78      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.015760064125061, 'eval_accuracy': 0.9831269560484419, 'eval_f1': 0.9831269560484419, 'eval_precision': 0.9831269560484419, 'eval_recall': 0.9831269560484419, 'eval_runtime': 69.306, 'eval_samples_per_second': 106.037, 'eval_steps_per_second': 3.319, 'epoch': 1.17}
{'loss': 0.3868, 'learning_rate': 6.989169675090252e-06, 'epoch': 1.26}
              precision    recall  f1-score   support

      NOT_QA       0.99      0.98      0.99      7154
      QA_NEW       0.48      0.56      0.52       195

    accuracy                           0.97      7349
   macro avg       0.73      0.77      0.75      7349
weighted avg       0.97      0.97      0.97      7349

{'eval_loss': 0.762325644493103, 'eval_accuracy': 0.9722411212409852, 'eval_f1': 0.9722411212409852, 'eval_precision': 0.9722411212409852, 'eval_recall': 0.9722411212409852, 'eval_runtime': 69.411, 'eval_samples_per_second': 105.877, 'eval_steps_per_second': 3.314, 'epoch': 1.26}
{'loss': 0.4037, 'learning_rate': 6.91696750902527e-06, 'epoch': 1.35}
              precision    recall  f1-score   support

      NOT_QA       0.99      0.98      0.99      7154
      QA_NEW       0.48      0.56      0.52       195

    accuracy                           0.97      7349
   macro avg       0.74      0.77      0.75      7349
weighted avg       0.97      0.97      0.97      7349

{'eval_loss': 0.9606295824050903, 'eval_accuracy': 0.9723771941760784, 'eval_f1': 0.9723771941760784, 'eval_precision': 0.9723771941760784, 'eval_recall': 0.9723771941760784, 'eval_runtime': 69.4834, 'eval_samples_per_second': 105.766, 'eval_steps_per_second': 3.31, 'epoch': 1.35}
{'loss': 0.3617, 'learning_rate': 6.844765342960288e-06, 'epoch': 1.44}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.83      0.41      0.55       195

    accuracy                           0.98      7349
   macro avg       0.91      0.70      0.77      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.053344488143921, 'eval_accuracy': 0.9821744455027895, 'eval_f1': 0.9821744455027895, 'eval_precision': 0.9821744455027895, 'eval_recall': 0.9821744455027895, 'eval_runtime': 69.5807, 'eval_samples_per_second': 105.618, 'eval_steps_per_second': 3.306, 'epoch': 1.44}
{'loss': 0.4449, 'learning_rate': 6.772563176895307e-06, 'epoch': 1.53}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.79      0.42      0.55       195

    accuracy                           0.98      7349
   macro avg       0.89      0.71      0.77      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 0.9604523181915283, 'eval_accuracy': 0.9816301537624167, 'eval_f1': 0.9816301537624167, 'eval_precision': 0.9816301537624167, 'eval_recall': 0.9816301537624167, 'eval_runtime': 108.3103, 'eval_samples_per_second': 67.851, 'eval_steps_per_second': 2.124, 'epoch': 1.53}
{'loss': 0.4087, 'learning_rate': 6.7003610108303244e-06, 'epoch': 1.62}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.93      0.35      0.51       195

    accuracy                           0.98      7349
   macro avg       0.96      0.67      0.75      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.2527693510055542, 'eval_accuracy': 0.9820383725676963, 'eval_f1': 0.9820383725676963, 'eval_precision': 0.9820383725676963, 'eval_recall': 0.9820383725676963, 'eval_runtime': 69.6182, 'eval_samples_per_second': 105.562, 'eval_steps_per_second': 3.304, 'epoch': 1.62}
{'train_runtime': 5548.7374, 'train_samples_per_second': 79.861, 'train_steps_per_second': 9.984, 'train_loss': 0.5220648210313585, 'epoch': 1.62}
              precision    recall  f1-score   support

      NOT_QA       0.99      1.00      0.99     13539
      QA_NEW       0.81      0.46      0.59       376

    accuracy                           0.98     13915
   macro avg       0.90      0.73      0.79     13915
weighted avg       0.98      0.98      0.98     13915

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
F1: 0.9824649658641753
END: Tue Jun 27 17:37:28 EEST 2023
