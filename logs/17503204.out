START: Tue Jun 27 15:47:02 EEST 2023
Namespace(train_set=['data/CORE-corpus/train.tsv.gz', 'data/FinCORE_full/train.tsv', 'data/SweCORE/swe_train.tsv', 'data/FreCORE/fre_train.tsv'], dev_set=['data/CORE-corpus/dev.tsv.gz', 'data/FinCORE_full/dev.tsv', 'data/SweCORE/swe_dev.tsv', 'data/FreCORE/fre_dev.tsv'], test_set=['data/CORE-corpus/test.tsv.gz', 'data/FinCORE_full/test.tsv', 'data/SweCORE/swe_test.tsv', 'data/FreCORE/fre_test.tsv'], model='xlm-roberta-base', batch=8, epochs=10, learning=5e-06, save=True, save_name='register-qa-binary-5e-6', weights=True)
Downloading and preparing dataset csv/default to /users/annieske/.cache/huggingface/datasets/csv/default-0e457d6a2127d4af/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Dataset csv downloaded and prepared to /users/annieske/.cache/huggingface/datasets/csv/default-0e457d6a2127d4af/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
Downloading and preparing dataset csv/default to /users/annieske/.cache/huggingface/datasets/csv/default-01629e9946cd6850/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Dataset csv downloaded and prepared to /users/annieske/.cache/huggingface/datasets/csv/default-01629e9946cd6850/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
Downloading and preparing dataset csv/default to /users/annieske/.cache/huggingface/datasets/csv/default-fe75b8fbfa0c5571/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Dataset csv downloaded and prepared to /users/annieske/.cache/huggingface/datasets/csv/default-fe75b8fbfa0c5571/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 45597
    })
    dev: Dataset({
        features: ['text', 'label'],
        num_rows: 7609
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 14377
    })
})
sub-register mapping
filtering
train 44313
test 13915
dev 7349
change labels to only qa
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
tokenization
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[0 1]
[ 0.51464508 17.57057891]
2
training
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.8412, 'learning_rate': 4.954873646209387e-06, 'epoch': 0.09}
              precision    recall  f1-score   support

      NOT_QA       0.97      1.00      0.99      7154
      QA_NEW       0.00      0.00      0.00       195

    accuracy                           0.97      7349
   macro avg       0.49      0.50      0.49      7349
weighted avg       0.95      0.97      0.96      7349

{'eval_loss': 1.9809008836746216, 'eval_accuracy': 0.9734657776568241, 'eval_f1': 0.9734657776568241, 'eval_precision': 0.9734657776568241, 'eval_recall': 0.9734657776568241, 'eval_runtime': 69.5079, 'eval_samples_per_second': 105.729, 'eval_steps_per_second': 3.309, 'epoch': 0.09}
{'loss': 0.9429, 'learning_rate': 4.909747292418773e-06, 'epoch': 0.18}
              precision    recall  f1-score   support

      NOT_QA       0.97      1.00      0.99      7154
      QA_NEW       0.00      0.00      0.00       195

    accuracy                           0.97      7349
   macro avg       0.49      0.50      0.49      7349
weighted avg       0.95      0.97      0.96      7349

{'eval_loss': 1.6386549472808838, 'eval_accuracy': 0.9734657776568241, 'eval_f1': 0.9734657776568241, 'eval_precision': 0.9734657776568241, 'eval_recall': 0.9734657776568241, 'eval_runtime': 69.2313, 'eval_samples_per_second': 106.151, 'eval_steps_per_second': 3.322, 'epoch': 0.18}
{'loss': 0.7784, 'learning_rate': 4.864620938628159e-06, 'epoch': 0.27}
              precision    recall  f1-score   support

      NOT_QA       0.97      1.00      0.99      7154
      QA_NEW       0.00      0.00      0.00       195

    accuracy                           0.97      7349
   macro avg       0.49      0.50      0.49      7349
weighted avg       0.95      0.97      0.96      7349

{'eval_loss': 1.592543363571167, 'eval_accuracy': 0.9734657776568241, 'eval_f1': 0.9734657776568241, 'eval_precision': 0.9734657776568241, 'eval_recall': 0.9734657776568241, 'eval_runtime': 69.2675, 'eval_samples_per_second': 106.096, 'eval_steps_per_second': 3.32, 'epoch': 0.27}
{'loss': 0.7739, 'learning_rate': 4.819494584837545e-06, 'epoch': 0.36}
              precision    recall  f1-score   support

      NOT_QA       0.97      1.00      0.99      7154
      QA_NEW       0.00      0.00      0.00       195

    accuracy                           0.97      7349
   macro avg       0.49      0.50      0.49      7349
weighted avg       0.95      0.97      0.96      7349

{'eval_loss': 1.8513565063476562, 'eval_accuracy': 0.9734657776568241, 'eval_f1': 0.9734657776568241, 'eval_precision': 0.9734657776568241, 'eval_recall': 0.9734657776568241, 'eval_runtime': 69.2954, 'eval_samples_per_second': 106.053, 'eval_steps_per_second': 3.319, 'epoch': 0.36}
{'loss': 0.6173, 'learning_rate': 4.774368231046932e-06, 'epoch': 0.45}
              precision    recall  f1-score   support

      NOT_QA       0.99      0.98      0.98      7154
      QA_NEW       0.36      0.50      0.42       195

    accuracy                           0.96      7349
   macro avg       0.67      0.74      0.70      7349
weighted avg       0.97      0.96      0.97      7349

{'eval_loss': 1.0216774940490723, 'eval_accuracy': 0.9631242345897401, 'eval_f1': 0.9631242345897401, 'eval_precision': 0.9631242345897401, 'eval_recall': 0.9631242345897401, 'eval_runtime': 69.7222, 'eval_samples_per_second': 105.404, 'eval_steps_per_second': 3.299, 'epoch': 0.45}
{'loss': 0.5099, 'learning_rate': 4.729241877256318e-06, 'epoch': 0.54}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.85      0.35      0.49       195

    accuracy                           0.98      7349
   macro avg       0.92      0.67      0.74      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.443350911140442, 'eval_accuracy': 0.9810858620220438, 'eval_f1': 0.9810858620220438, 'eval_precision': 0.9810858620220438, 'eval_recall': 0.9810858620220438, 'eval_runtime': 69.3619, 'eval_samples_per_second': 105.951, 'eval_steps_per_second': 3.316, 'epoch': 0.54}
{'loss': 0.468, 'learning_rate': 4.684115523465704e-06, 'epoch': 0.63}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.81      0.38      0.52       195

    accuracy                           0.98      7349
   macro avg       0.89      0.69      0.76      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.2032561302185059, 'eval_accuracy': 0.981221934957137, 'eval_f1': 0.9812219349571372, 'eval_precision': 0.981221934957137, 'eval_recall': 0.981221934957137, 'eval_runtime': 69.3954, 'eval_samples_per_second': 105.9, 'eval_steps_per_second': 3.314, 'epoch': 0.63}
{'loss': 0.5016, 'learning_rate': 4.63898916967509e-06, 'epoch': 0.72}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.83      0.41      0.55       195

    accuracy                           0.98      7349
   macro avg       0.91      0.70      0.77      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 0.9960673451423645, 'eval_accuracy': 0.9821744455027895, 'eval_f1': 0.9821744455027895, 'eval_precision': 0.9821744455027895, 'eval_recall': 0.9821744455027895, 'eval_runtime': 69.499, 'eval_samples_per_second': 105.743, 'eval_steps_per_second': 3.309, 'epoch': 0.72}
{'loss': 0.5399, 'learning_rate': 4.5938628158844765e-06, 'epoch': 0.81}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.77      0.43      0.55       195

    accuracy                           0.98      7349
   macro avg       0.88      0.71      0.77      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 0.9211166501045227, 'eval_accuracy': 0.9814940808273235, 'eval_f1': 0.9814940808273235, 'eval_precision': 0.9814940808273235, 'eval_recall': 0.9814940808273235, 'eval_runtime': 69.4901, 'eval_samples_per_second': 105.756, 'eval_steps_per_second': 3.31, 'epoch': 0.81}
{'loss': 0.4927, 'learning_rate': 4.548736462093864e-06, 'epoch': 0.9}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.87      0.37      0.52       195

    accuracy                           0.98      7349
   macro avg       0.93      0.69      0.76      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.080641508102417, 'eval_accuracy': 0.9819022996326031, 'eval_f1': 0.9819022996326031, 'eval_precision': 0.9819022996326031, 'eval_recall': 0.9819022996326031, 'eval_runtime': 69.4237, 'eval_samples_per_second': 105.857, 'eval_steps_per_second': 3.313, 'epoch': 0.9}
{'loss': 0.5299, 'learning_rate': 4.50361010830325e-06, 'epoch': 0.99}
              precision    recall  f1-score   support

      NOT_QA       0.99      0.99      0.99      7154
      QA_NEW       0.65      0.48      0.55       195

    accuracy                           0.98      7349
   macro avg       0.82      0.74      0.77      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 0.8467418551445007, 'eval_accuracy': 0.9794529868009253, 'eval_f1': 0.9794529868009253, 'eval_precision': 0.9794529868009253, 'eval_recall': 0.9794529868009253, 'eval_runtime': 69.5861, 'eval_samples_per_second': 105.61, 'eval_steps_per_second': 3.305, 'epoch': 0.99}
{'loss': 0.3993, 'learning_rate': 4.458483754512636e-06, 'epoch': 1.08}
              precision    recall  f1-score   support

      NOT_QA       0.98      1.00      0.99      7154
      QA_NEW       0.79      0.43      0.56       195

    accuracy                           0.98      7349
   macro avg       0.88      0.71      0.77      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 1.0199813842773438, 'eval_accuracy': 0.9817662266975099, 'eval_f1': 0.9817662266975099, 'eval_precision': 0.9817662266975099, 'eval_recall': 0.9817662266975099, 'eval_runtime': 69.593, 'eval_samples_per_second': 105.6, 'eval_steps_per_second': 3.305, 'epoch': 1.08}
{'loss': 0.4405, 'learning_rate': 4.413357400722022e-06, 'epoch': 1.17}
              precision    recall  f1-score   support

      NOT_QA       0.99      0.99      0.99      7154
      QA_NEW       0.61      0.52      0.57       195

    accuracy                           0.98      7349
   macro avg       0.80      0.76      0.78      7349
weighted avg       0.98      0.98      0.98      7349

{'eval_loss': 0.8635787963867188, 'eval_accuracy': 0.978636549190366, 'eval_f1': 0.978636549190366, 'eval_precision': 0.978636549190366, 'eval_recall': 0.978636549190366, 'eval_runtime': 69.5784, 'eval_samples_per_second': 105.622, 'eval_steps_per_second': 3.306, 'epoch': 1.17}
{'train_runtime': 3378.1433, 'train_samples_per_second': 131.176, 'train_steps_per_second': 16.4, 'train_loss': 0.6027261376014122, 'epoch': 1.17}
              precision    recall  f1-score   support

      NOT_QA       0.99      1.00      0.99     13539
      QA_NEW       0.74      0.46      0.57       376

    accuracy                           0.98     13915
   macro avg       0.86      0.73      0.78     13915
weighted avg       0.98      0.98      0.98     13915

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
F1: 0.981099532878189
END: Tue Jun 27 16:53:32 EEST 2023
