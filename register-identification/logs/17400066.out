START: ti 20.6.2023 14.51.43 +0300
Namespace(train_set=['data/FinCORE_full/train.tsv', 'data/CORE-corpus/train.tsv.gz'], dev_set=['data/FinCORE_full/dev.tsv', 'data/CORE-corpus/dev.tsv.gz'], test_set=['data/FinCORE_full/test.tsv', 'data/CORE-corpus/test.tsv.gz'], model='xlm-roberta-base', threshold=0.5, batch=8, epochs=4, learning=8e-06, save=True, weights=True)
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 41440
    })
    dev: Dataset({
        features: ['text', 'label'],
        num_rows: 5920
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 11843
    })
})
sub-register mapping
filtering
train 40456
test 11561
dev 5780
binarization
tokenization
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[('IN', tensor(2.4753, device='cuda:0')), ('NA', tensor(1.4243, device='cuda:0')), ('HI', tensor(0.4468, device='cuda:0')), ('LY', tensor(1.9104, device='cuda:0')), ('IP', tensor(9.0627, device='cuda:0')), ('SP', tensor(0.2538, device='cuda:0')), ('ID', tensor(0.5443, device='cuda:0')), ('OP', tensor(3.7335, device='cuda:0')), ('QA_NEW', tensor(23.8257, device='cuda:0'))]
9
training
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'loss': 0.4415, 'learning_rate': 7.802254300968954e-06, 'epoch': 0.1}
              precision    recall  f1-score   support

          IN       0.00      0.00      0.00       252
          NA       0.00      0.00      0.00       454
          HI       0.00      0.00      0.00      1444
          LY       0.00      0.00      0.00       331
          IP       0.00      0.00      0.00        69
          SP       0.00      0.00      0.00      2534
          ID       0.00      0.00      0.00      1167
          OP       0.00      0.00      0.00       171
      QA_NEW       0.00      0.00      0.00        81

   micro avg       0.00      0.00      0.00      6503
   macro avg       0.00      0.00      0.00      6503
weighted avg       0.00      0.00      0.00      6503
 samples avg       0.00      0.00      0.00      6503

{'eval_loss': 0.41007164120674133, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_roc_auc': 0.5, 'eval_accuracy': 0.010034602076124567, 'eval_runtime': 61.7708, 'eval_samples_per_second': 93.572, 'eval_steps_per_second': 2.93, 'epoch': 0.1}
{'loss': 0.3904, 'learning_rate': 7.604508601937907e-06, 'epoch': 0.2}
              precision    recall  f1-score   support

          IN       0.41      0.22      0.28       252
          NA       0.63      0.30      0.41       454
          HI       0.00      0.00      0.00      1444
          LY       1.00      0.02      0.04       331
          IP       0.75      0.74      0.74        69
          SP       0.00      0.00      0.00      2534
          ID       0.00      0.00      0.00      1167
          OP       0.50      0.01      0.01       171
      QA_NEW       0.00      0.00      0.00        81

   micro avg       0.59      0.04      0.07      6503
   macro avg       0.37      0.14      0.17      6503
weighted avg       0.13      0.04      0.05      6503
 samples avg       0.04      0.04      0.03      6503

{'eval_loss': 0.32716700434684753, 'eval_f1': 0.07241777264858627, 'eval_precision': 0.585081585081585, 'eval_recall': 0.038597570352145165, 'eval_roc_auc': 0.5173434717766833, 'eval_accuracy': 0.02889273356401384, 'eval_runtime': 54.2456, 'eval_samples_per_second': 106.552, 'eval_steps_per_second': 3.337, 'epoch': 0.2}
{'loss': 0.3094, 'learning_rate': 7.406762902906862e-06, 'epoch': 0.3}
              precision    recall  f1-score   support

          IN       0.66      0.33      0.44       252
          NA       0.68      0.59      0.63       454
          HI       0.00      0.00      0.00      1444
          LY       0.87      0.18      0.29       331
          IP       1.00      0.70      0.82        69
          SP       0.94      0.37      0.53      2534
          ID       0.00      0.00      0.00      1167
          OP       0.85      0.37      0.51       171
      QA_NEW       0.56      0.62      0.59        81

   micro avg       0.84      0.23      0.36      6503
   macro avg       0.62      0.35      0.42      6503
weighted avg       0.52      0.23      0.31      6503
 samples avg       0.25      0.24      0.24      6503

{'eval_loss': 0.2556109130382538, 'eval_f1': 0.36186113789778207, 'eval_precision': 0.8371444506413832, 'eval_recall': 0.2308165462094418, 'eval_roc_auc': 0.6122006803371836, 'eval_accuracy': 0.23200692041522492, 'eval_runtime': 54.2874, 'eval_samples_per_second': 106.47, 'eval_steps_per_second': 3.334, 'epoch': 0.3}
{'loss': 0.2492, 'learning_rate': 7.209017203875816e-06, 'epoch': 0.4}
              precision    recall  f1-score   support

          IN       0.91      0.08      0.15       252
          NA       0.77      0.67      0.72       454
          HI       0.93      0.06      0.12      1444
          LY       0.74      0.33      0.45       331
          IP       0.96      0.78      0.86        69
          SP       0.91      0.49      0.64      2534
          ID       0.54      0.01      0.01      1167
          OP       0.71      0.50      0.59       171
      QA_NEW       0.83      0.30      0.44        81

   micro avg       0.86      0.30      0.44      6503
   macro avg       0.81      0.36      0.44      6503
weighted avg       0.82      0.30      0.39      6503
 samples avg       0.32      0.31      0.31      6503

{'eval_loss': 0.26855963468551636, 'eval_f1': 0.443607305936073, 'eval_precision': 0.8608772707133363, 'eval_recall': 0.29878517607258187, 'eval_roc_auc': 0.6459433273205145, 'eval_accuracy': 0.30017301038062283, 'eval_runtime': 54.3413, 'eval_samples_per_second': 106.365, 'eval_steps_per_second': 3.331, 'epoch': 0.4}
{'loss': 0.2335, 'learning_rate': 7.011271504844769e-06, 'epoch': 0.49}
              precision    recall  f1-score   support

          IN       0.62      0.55      0.58       252
          NA       0.89      0.56      0.69       454
          HI       0.88      0.35      0.50      1444
          LY       0.66      0.47      0.55       331
          IP       0.96      0.75      0.85        69
          SP       0.86      0.66      0.74      2534
          ID       0.65      0.49      0.56      1167
          OP       0.93      0.44      0.60       171
      QA_NEW       0.69      0.58      0.63        81

   micro avg       0.80      0.53      0.64      6503
   macro avg       0.79      0.54      0.63      6503
weighted avg       0.81      0.53      0.63      6503
 samples avg       0.58      0.55      0.56      6503

{'eval_loss': 0.2292887270450592, 'eval_f1': 0.6378228782287823, 'eval_precision': 0.7970947659672585, 'eval_recall': 0.5316007996309395, 'eval_roc_auc': 0.756133681885905, 'eval_accuracy': 0.5198961937716263, 'eval_runtime': 54.4279, 'eval_samples_per_second': 106.196, 'eval_steps_per_second': 3.326, 'epoch': 0.49}
{'loss': 0.2186, 'learning_rate': 6.813525805813723e-06, 'epoch': 0.59}
              precision    recall  f1-score   support

          IN       0.60      0.61      0.60       252
          NA       0.91      0.54      0.68       454
          HI       0.89      0.31      0.46      1444
          LY       0.60      0.53      0.57       331
          IP       0.89      0.81      0.85        69
          SP       0.84      0.71      0.77      2534
          ID       0.73      0.36      0.48      1167
          OP       0.83      0.50      0.63       171
      QA_NEW       0.60      0.60      0.60        81

   micro avg       0.80      0.53      0.64      6503
   macro avg       0.77      0.55      0.63      6503
weighted avg       0.81      0.53      0.62      6503
 samples avg       0.58      0.55      0.56      6503

{'eval_loss': 0.22389431297779083, 'eval_f1': 0.6373830043554813, 'eval_precision': 0.8020055970149254, 'eval_recall': 0.5288328463785945, 'eval_roc_auc': 0.7550902373686148, 'eval_accuracy': 0.5214532871972318, 'eval_runtime': 54.5494, 'eval_samples_per_second': 105.959, 'eval_steps_per_second': 3.318, 'epoch': 0.59}
{'loss': 0.2171, 'learning_rate': 6.615780106782678e-06, 'epoch': 0.69}
              precision    recall  f1-score   support

          IN       0.58      0.65      0.62       252
          NA       0.88      0.63      0.74       454
          HI       0.80      0.46      0.58      1444
          LY       0.58      0.60      0.59       331
          IP       0.92      0.83      0.87        69
          SP       0.92      0.56      0.70      2534
          ID       0.72      0.39      0.51      1167
          OP       0.73      0.53      0.61       171
      QA_NEW       0.60      0.67      0.63        81

   micro avg       0.80      0.52      0.63      6503
   macro avg       0.75      0.59      0.65      6503
weighted avg       0.82      0.52      0.63      6503
 samples avg       0.57      0.54      0.55      6503

{'eval_loss': 0.2054942548274994, 'eval_f1': 0.6304307290695506, 'eval_precision': 0.8006156760596732, 'eval_recall': 0.5199138858988159, 'eval_roc_auc': 0.7507076514758926, 'eval_accuracy': 0.5060553633217993, 'eval_runtime': 54.6584, 'eval_samples_per_second': 105.748, 'eval_steps_per_second': 3.311, 'epoch': 0.69}
{'loss': 0.2081, 'learning_rate': 6.418034407751631e-06, 'epoch': 0.79}
              precision    recall  f1-score   support

          IN       0.55      0.65      0.60       252
          NA       0.84      0.69      0.76       454
          HI       0.80      0.50      0.62      1444
          LY       0.63      0.56      0.59       331
          IP       0.87      0.86      0.86        69
          SP       0.95      0.42      0.58      2534
          ID       0.79      0.31      0.44      1167
          OP       0.60      0.61      0.61       171
      QA_NEW       0.59      0.68      0.63        81

   micro avg       0.80      0.47      0.59      6503
   macro avg       0.74      0.59      0.63      6503
weighted avg       0.83      0.47      0.58      6503
 samples avg       0.50      0.48      0.48      6503

{'eval_loss': 0.20195819437503815, 'eval_f1': 0.5884069247228165, 'eval_precision': 0.8004763164858428, 'eval_recall': 0.46516992157465786, 'eval_roc_auc': 0.7243023411067699, 'eval_accuracy': 0.4472318339100346, 'eval_runtime': 54.7069, 'eval_samples_per_second': 105.654, 'eval_steps_per_second': 3.309, 'epoch': 0.79}
{'loss': 0.2329, 'learning_rate': 6.220288708720585e-06, 'epoch': 0.89}
              precision    recall  f1-score   support

          IN       0.66      0.55      0.60       252
          NA       0.76      0.78      0.77       454
          HI       0.84      0.48      0.61      1444
          LY       0.67      0.53      0.59       331
          IP       0.92      0.83      0.87        69
          SP       0.90      0.65      0.75      2534
          ID       0.74      0.43      0.55      1167
          OP       0.55      0.61      0.58       171
      QA_NEW       0.51      0.70      0.59        81

   micro avg       0.80      0.57      0.67      6503
   macro avg       0.73      0.62      0.66      6503
weighted avg       0.81      0.57      0.67      6503
 samples avg       0.62      0.59      0.60      6503

{'eval_loss': 0.18412064015865326, 'eval_f1': 0.6691209481907157, 'eval_precision': 0.8040569702201122, 'eval_recall': 0.5729663232354298, 'eval_roc_auc': 0.7765088662994821, 'eval_accuracy': 0.5501730103806228, 'eval_runtime': 54.7797, 'eval_samples_per_second': 105.514, 'eval_steps_per_second': 3.304, 'epoch': 0.89}
{'loss': 0.2043, 'learning_rate': 6.022543009689539e-06, 'epoch': 0.99}
              precision    recall  f1-score   support

          IN       0.52      0.71      0.60       252
          NA       0.86      0.70      0.77       454
          HI       0.84      0.48      0.61      1444
          LY       0.78      0.46      0.58       331
          IP       0.89      0.84      0.87        69
          SP       0.89      0.67      0.76      2534
          ID       0.80      0.31      0.45      1167
          OP       0.63      0.60      0.61       171
      QA_NEW       0.53      0.68      0.60        81

   micro avg       0.82      0.56      0.66      6503
   macro avg       0.75      0.60      0.65      6503
weighted avg       0.83      0.56      0.65      6503
 samples avg       0.60      0.58      0.58      6503

{'eval_loss': 0.19595366716384888, 'eval_f1': 0.6611721611721612, 'eval_precision': 0.8172968077880914, 'eval_recall': 0.5551284022758727, 'eval_roc_auc': 0.7686993814002558, 'eval_accuracy': 0.5346020761245674, 'eval_runtime': 54.7889, 'eval_samples_per_second': 105.496, 'eval_steps_per_second': 3.304, 'epoch': 0.99}
{'loss': 0.2024, 'learning_rate': 5.824797310658493e-06, 'epoch': 1.09}
              precision    recall  f1-score   support

          IN       0.60      0.64      0.62       252
          NA       0.83      0.71      0.77       454
          HI       0.83      0.46      0.59      1444
          LY       0.80      0.42      0.55       331
          IP       0.91      0.86      0.88        69
          SP       0.92      0.60      0.73      2534
          ID       0.70      0.51      0.59      1167
          OP       0.65      0.63      0.64       171
      QA_NEW       0.46      0.78      0.58        81

   micro avg       0.81      0.56      0.66      6503
   macro avg       0.74      0.62      0.66      6503
weighted avg       0.82      0.56      0.66      6503
 samples avg       0.61      0.58      0.59      6503

{'eval_loss': 0.18204320967197418, 'eval_f1': 0.6596421110000908, 'eval_precision': 0.8058144695960942, 'eval_recall': 0.5583576810702753, 'eval_roc_auc': 0.7695670471392635, 'eval_accuracy': 0.5410034602076125, 'eval_runtime': 54.7694, 'eval_samples_per_second': 105.533, 'eval_steps_per_second': 3.305, 'epoch': 1.09}
{'loss': 0.1909, 'learning_rate': 5.627051611627447e-06, 'epoch': 1.19}
              precision    recall  f1-score   support

          IN       0.56      0.67      0.61       252
          NA       0.77      0.79      0.78       454
          HI       0.82      0.48      0.60      1444
          LY       0.65      0.56      0.60       331
          IP       0.91      0.86      0.88        69
          SP       0.92      0.61      0.73      2534
          ID       0.76      0.39      0.52      1167
          OP       0.49      0.69      0.57       171
      QA_NEW       0.57      0.72      0.63        81

   micro avg       0.79      0.56      0.66      6503
   macro avg       0.72      0.64      0.66      6503
weighted avg       0.81      0.56      0.65      6503
 samples avg       0.60      0.58      0.58      6503

{'eval_loss': 0.18499305844306946, 'eval_f1': 0.6556106354213609, 'eval_precision': 0.7920296167247387, 'eval_recall': 0.5592803321543903, 'eval_roc_auc': 0.7691495801422698, 'eval_accuracy': 0.529757785467128, 'eval_runtime': 54.8463, 'eval_samples_per_second': 105.385, 'eval_steps_per_second': 3.3, 'epoch': 1.19}
{'loss': 0.1779, 'learning_rate': 5.429305912596401e-06, 'epoch': 1.29}
              precision    recall  f1-score   support

          IN       0.71      0.57      0.63       252
          NA       0.87      0.70      0.77       454
          HI       0.78      0.53      0.63      1444
          LY       0.84      0.39      0.54       331
          IP       0.92      0.84      0.88        69
          SP       0.94      0.49      0.65      2534
          ID       0.67      0.57      0.62      1167
          OP       0.66      0.61      0.64       171
      QA_NEW       0.61      0.68      0.64        81

   micro avg       0.80      0.54      0.64      6503
   macro avg       0.78      0.60      0.67      6503
weighted avg       0.83      0.54      0.64      6503
 samples avg       0.58      0.55      0.56      6503

{'eval_loss': 0.2066919207572937, 'eval_f1': 0.6427779830070188, 'eval_precision': 0.8046242774566474, 'eval_recall': 0.5351376287867138, 'eval_roc_auc': 0.758286568199627, 'eval_accuracy': 0.5166089965397924, 'eval_runtime': 54.8211, 'eval_samples_per_second': 105.434, 'eval_steps_per_second': 3.302, 'epoch': 1.29}
{'loss': 0.1889, 'learning_rate': 5.231560213565354e-06, 'epoch': 1.38}
              precision    recall  f1-score   support

          IN       0.67      0.56      0.61       252
          NA       0.85      0.71      0.77       454
          HI       0.84      0.44      0.57      1444
          LY       0.82      0.40      0.54       331
          IP       0.94      0.86      0.89        69
          SP       0.93      0.59      0.72      2534
          ID       0.74      0.41      0.52      1167
          OP       0.73      0.57      0.64       171
      QA_NEW       0.61      0.74      0.67        81

   micro avg       0.84      0.53      0.65      6503
   macro avg       0.79      0.58      0.66      6503
weighted avg       0.84      0.53      0.64      6503
 samples avg       0.57      0.55      0.55      6503

{'eval_loss': 0.19465918838977814, 'eval_f1': 0.6471480007580065, 'eval_precision': 0.8430017279684029, 'eval_recall': 0.5251422420421344, 'eval_roc_auc': 0.7555847203356091, 'eval_accuracy': 0.5173010380622838, 'eval_runtime': 54.834, 'eval_samples_per_second': 105.409, 'eval_steps_per_second': 3.301, 'epoch': 1.38}
{'train_runtime': 3266.3678, 'train_samples_per_second': 49.542, 'train_steps_per_second': 6.193, 'train_loss': 0.24749553898402624, 'epoch': 1.38}
              precision    recall  f1-score   support

          IN       0.69      0.55      0.61       528
          NA       0.78      0.80      0.79       917
          HI       0.83      0.48      0.61      2899
          LY       0.67      0.54      0.60       652
          IP       0.94      0.79      0.86       140
          SP       0.89      0.66      0.76      5018
          ID       0.74      0.42      0.54      2366
          OP       0.62      0.64      0.63       348
      QA_NEW       0.54      0.72      0.61       164

   micro avg       0.81      0.58      0.67     13032
   macro avg       0.74      0.62      0.67     13032
weighted avg       0.81      0.58      0.67     13032
 samples avg       0.63      0.60      0.61     13032

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
F1: 0.6740889783143306
saved
END: ti 20.6.2023 15.50.53 +0300
